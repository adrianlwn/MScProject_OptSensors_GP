
@article{arcucci_optimal_2019,
	title = {Optimal reduced space for {Variational} {Data} {Assimilation}},
	volume = {379},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999118307095},
	doi = {10.1016/j.jcp.2018.10.042},
	abstract = {Data Assimilation (DA) is an uncertainty quantiﬁcation technique used to incorporate observed data into a prediction model in order to improve numerical forecasted results. Variational DA (VarDA) is based on the minimisation of a function which estimates the discrepancy between numerical results and observations. Operational forecasting requires real-time data assimilation. This mandates the choice of opportune methods to improve the eﬃciency of VarDA codes without loosing accuracy. Due to the scale of the forecasting area and the number of state variables used to describe the physical model, DA is a big data problem. In this paper, the Truncated Singular Value Decomposition (TSVD) is used to reduce the space dimension, alleviate the computational cost and reduce the errors. Nevertheless, a consequence is that important information is lost if the truncation parameter is not properly chosen. We provide an algorithm to compute the optimal truncation parameter and we prove that the optimal estimation reduces the illconditioning and removes the statistically less signiﬁcant modes which could add noise to the estimate obtained from DA. In this paper, numerical issues faced in developing VarDA algorithm include the ill-conditioning of the background covariance matrix, the choice of a preconditioning and the choice of the regularisation parameter. We also show how the choice of the regularisation parameter impacts on the eﬃciency of the VarDA minimisation computed by the L-BFGS (Limited – Broyden Fletcher Goldfarb Shanno). Experimental results are provided for pollutant dispersion within an urban environment.},
	language = {en},
	urldate = {2019-04-15},
	journal = {Journal of Computational Physics},
	author = {Arcucci, Rossella and Mottet, Laetitia and Pain, Christopher and Guo, Yi-Ke},
	month = feb,
	year = {2019},
	pages = {51--69},
	file = {Arcucci et al. - 2019 - Optimal reduced space for Variational Data Assimil.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/initial papers/Arcucci et al. - 2019 - Optimal reduced space for Variational Data Assimil.pdf:application/pdf}
}

@article{krause_near-optimal_2008,
	title = {Near-{Optimal} {Sensor} {Placements} in {Gaussian} {Processes}: {Theory}, {Efﬁcient} {Algorithms} and {Empirical} {Studies}},
	abstract = {When monitoring spatial phenomena, which can often be modeled as Gaussian processes (GPs), choosing sensor locations is a fundamental task. There are several common strategies to address this task, for example, geometry or disk models, placing sensors at the points of highest entropy (variance) in the GP model, and A-, D-, or E-optimal design. In this paper, we tackle the combinatorial optimization problem of maximizing the mutual information between the chosen locations and the locations which are not selected. We prove that the problem of ﬁnding the conﬁguration that maximizes mutual information is NP-complete. To address this issue, we describe a polynomial-time approximation that is within (1 − 1/e) of the optimum by exploiting the submodularity of mutual information. We also show how submodularity can be used to obtain online bounds, and design branch and bound search procedures. We then extend our algorithm to exploit lazy evaluations and local structure in the GP, yielding signiﬁcant speedups. We also extend our approach to ﬁnd placements which are robust against node failures and uncertainties in the model. These extensions are again associated with rigorous theoretical approximation guarantees, exploiting the submodularity of the objective function. We demonstrate the advantages of our approach towards optimizing mutual information in a very extensive empirical study on two real-world data sets.},
	language = {en},
	author = {Krause, Andreas and Singh, Ajit and Guestrin, Carlos},
	year = {2008},
	pages = {50},
	file = {Krause et al. - 2008 - Near-Optimal Sensor Placements in Gaussian Process.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/initial papers/Krause et al. - 2008 - Near-Optimal Sensor Placements in Gaussian Process.pdf:application/pdf}
}

@article{song_natural_2018,
	title = {Natural ventilation in cities: the implications of fluid mechanics},
	volume = {46},
	issn = {0961-3218, 1466-4321},
	shorttitle = {Natural ventilation in cities},
	url = {https://www.tandfonline.com/doi/full/10.1080/09613218.2018.1468158},
	doi = {10.1080/09613218.2018.1468158},
	abstract = {Research under the Managing Air for Green Inner Cities (MAGIC) project uses measurements and modelling to investigate the connections between external and internal conditions: the impact of urban airﬂow on the natural ventilation of a building. The test site was chosen so that under diﬀerent environmental conditions the levels of external pollutants entering the building, from either a polluted road or a relatively clean courtyard, would be signiﬁcantly diﬀerent. Measurements included temperature, relative humidity, local wind and solar radiation, together with levels of carbon monoxide (CO) and carbon dioxide (CO2) both inside and outside the building to assess the indoor–outdoor exchange ﬂows. Building ventilation took place through windows on two sides, allowing for single-sided and crosswind-driven ventilation, and also stackdriven ventilation in low wind conditions. The external ﬂow around the test site was modelled in an urban boundary layer in a wind tunnel. The wind tunnel results were incorporated in a largeeddy-simulation model, Fluidity, and the results compared with monitoring data taken both within the building and from the surrounding area. In particular, the eﬀects of street layout and associated street canyons, of roof geometry and the wakes of nearby tall buildings were examined.},
	language = {en},
	number = {8},
	urldate = {2019-04-15},
	journal = {Building Research \& Information},
	author = {Song, Jiyun and Fan, S. and Lin, W. and Mottet, L. and Woodward, H. and Davies Wykes, M. and Arcucci, R. and Xiao, D. and Debay, J.-E. and ApSimon, H. and Aristodemou, E. and Birch, D. and Carpentieri, M. and Fang, F. and Herzog, M. and Hunt, G. R. and Jones, R. L. and Pain, C. and Pavlidis, D. and Robins, A. G. and Short, C. A. and Linden, P. F.},
	month = nov,
	year = {2018},
	pages = {809--828},
	file = {Natural ventilation in cities the implications of fluid mechanics.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/initial papers/Natural ventilation in cities the implications of fluid mechanics.pdf:application/pdf}
}

@inproceedings{guestrin_near-optimal_2005,
	address = {Bonn, Germany},
	title = {Near-optimal sensor placements in {Gaussian} processes},
	isbn = {978-1-59593-180-1},
	url = {http://portal.acm.org/citation.cfm?doid=1102351.1102385},
	doi = {10.1145/1102351.1102385},
	abstract = {When monitoring spatial phenomena, which are often modeled as Gaussian Processes (GPs), choosing sensor locations is a fundamental task. A common strategy is to place sensors at the points of highest entropy (variance) in the GP model. We propose a mutual information criteria, and show that it produces better placements. Furthermore, we prove that ﬁnding the conﬁguration that maximizes mutual information is NP-complete. To address this issue, we describe a polynomial-time approximation that is within (1 − 1/e) of the optimum by exploiting the submodularity of our criterion. This algorithm is extended to handle local structure in the GP, yielding signiﬁcant speedups. We demonstrate the advantages of our approach on two real-world data sets.},
	language = {en},
	urldate = {2019-04-15},
	booktitle = {Proceedings of the 22nd international conference on {Machine} learning  - {ICML} '05},
	publisher = {ACM Press},
	author = {Guestrin, Carlos and Krause, Andreas and Singh, Ajit Paul},
	year = {2005},
	pages = {265--272},
	file = {Guestrin et al. - 2005 - Near-optimal sensor placements in Gaussian process.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/initial papers/Guestrin et al. - 2005 - Near-optimal sensor placements in Gaussian process.pdf:application/pdf}
}

@article{law_data_2015,
	title = {Data {Assimilation}: {A} {Mathematical} {Introduction}},
	shorttitle = {Data {Assimilation}},
	url = {http://arxiv.org/abs/1506.07825},
	abstract = {These notes provide a systematic mathematical treatment of the subject of data assimilation.},
	urldate = {2019-04-15},
	journal = {arXiv:1506.07825 [math, stat]},
	author = {Law, K. J. H. and Stuart, A. M. and Zygalakis, K. C.},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.07825},
	keywords = {Mathematics - Dynamical Systems, Mathematics - Optimization and Control, Statistics - Methodology},
	file = {arXiv.org Snapshot:/Users/adrian/Zotero/storage/LKCPR6YU/1506.html:text/html;Law et al. - 2015 - Data Assimilation A Mathematical Introduction.pdf:/Users/adrian/Zotero/storage/SPAUV7FU/Law et al. - 2015 - Data Assimilation A Mathematical Introduction.pdf:application/pdf}
}

@article{arcucci_variational_2017,
	title = {On the variational data assimilation problem solving and sensitivity analysis},
	volume = {335},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999117300505},
	doi = {10.1016/j.jcp.2017.01.034},
	abstract = {We consider the Variational Data Assimilation (VarDA) problem in an operational framework, namely, as it results when it is employed for the analysis of temperature and salinity variations of data collected in closed and semi closed seas. We present a computing approach to solve the main computational kernel at the heart of the VarDA problem, which outperforms the technique nowadays employed by the oceanographic operative software. The new approach is obtained by means of Tikhonov regularization. We provide the sensitivity analysis of this approach and we also study its performance in terms of the accuracy gain on the computed solution. We provide validations on two realistic oceanographic data sets.},
	language = {en},
	urldate = {2019-04-20},
	journal = {Journal of Computational Physics},
	author = {Arcucci, Rossella and D'Amore, Luisa and Pistoia, Jenny and Toumi, Ralf and Murli, Almerico},
	month = apr,
	year = {2017},
	pages = {311--326},
	file = {Arcucci et al. - 2017 - On the variational data assimilation problem solvi.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Data Assimilation/Arcucci et al. - 2017 - On the variational data assimilation problem solvi.pdf:application/pdf}
}

@article{kouichi_optimization_2016,
	series = {Air {Pollution} {XXIV}},
	title = {Optimization of sensor networks for the estimation of atmospheric pollutants sources},
	volume = {207},
	url = {https://hal.archives-ouvertes.fr/hal-01593828},
	doi = {10.2495/AIR160021},
	abstract = {This study describes a process to design a sensor network. This network could include: wireless mobile sensors deployed by first responders in hazardous material operations, stationary sensors used to protect an area against accidental, or intentional, contaminations or stationary air quality monitoring stations. The objective of the network is the estimation (localization – quantification) of releases sources. The design of such a network has an important issue in determining the optimal placement of sensors. This paper presents the first application of the renormalized data assimilation method to address this issue. It is associated with a classical optimization algorithm (simulate annealing) to solve the combinatory optimization problem consisting of finding the optimal configuration of sensors among a set of potential positions. Three scenarios, corresponding with three different cost functions, are proposed. The first one consists of optimizing the design of a network deployed in emergency situations. Experimental data from a wind tunnel experiment are used. The objective is to characterize the source to minimize error in measurement forecasts. The second one is to optimize the design of the same network but in a situation where the source can be anywhere in the domain. To that end, an entropic criterion is used. The last one consists of optimizing the design of a stationary network. The objective is to characterize the source with varying meteorological conditions (experimental meteorological data are used).},
	urldate = {2019-04-23},
	journal = {WIT Transactions on Ecology and the Environment},
	author = {kouichi, hamza and Turbelin, G and Ngae, P and Feiz, A. A. and Barbosa, E and Chpoun, A.},
	year = {2016},
	keywords = {network optimization, renormalized data assimilation, source characterization},
	pages = {11--21},
	file = {HAL PDF Full Text:/Users/adrian/Zotero/storage/F4NSMCG7/kouichi et al. - 2016 - Optimization of sensor networks for the estimation.pdf:application/pdf}
}

@article{nott_estimation_2002,
	title = {Estimation of nonstationary spatial covariance structure},
	volume = {89},
	issn = {0006-3444, 1464-3510},
	url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/89.4.819},
	doi = {10.1093/biomet/89.4.819},
	language = {en},
	number = {4},
	urldate = {2019-04-26},
	journal = {Biometrika},
	author = {Nott, D. J.},
	month = dec,
	year = {2002},
	pages = {819--829},
	file = {Nott - 2002 - Estimation of nonstationary spatial covariance str.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project/MScProject_OptSensors_GP/Gaussian Processes/Nott - 2002 - Estimation of nonstationary spatial covariance str.pdf:application/pdf}
}

@article{caselton_optimal_1984,
	title = {Optimal monitoring network designs},
	volume = {2},
	issn = {01677152},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0167715284900208},
	doi = {10.1016/0167-7152(84)90020-8},
	abstract = {The selection of a monitoring network is formulated as a decision problem whose solutions would then be optimal. The theory is apphed where the underlying field has a multivariate normal probability structure.},
	language = {en},
	number = {4},
	urldate = {2019-04-26},
	journal = {Statistics \& Probability Letters},
	author = {Caselton, W.F. and Zidek, J.V.},
	month = aug,
	year = {1984},
	pages = {223--227},
	file = {Caselton and Zidek - 1984 - Optimal monitoring network designs.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project/MScProject_OptSensors_GP/Gaussian Processes/Caselton and Zidek - 1984 - Optimal monitoring network designs.pdf:application/pdf}
}

@misc{noauthor_mutual_2019,
	title = {Mutual information},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Mutual_information&oldid=892560218},
	abstract = {In probability theory and information theory, the mutual information (MI) of two random variables is a measure of the mutual dependence between the two variables. More specifically, it quantifies the "amount of information" (in units such as shannons, commonly called bits) obtained about one random variable through observing the other random variable.  The concept of mutual information is intricately linked to that of entropy of a random variable, a fundamental notion in information theory that quantifies the expected "amount of information" held in a random variable.
Not limited to real-valued random variables like the correlation coefficient, MI is more general and determines how similar the joint distribution of the pair 
  
    
      
        (
        X
        ,
        Y
        )
      
    
    \{{\textbackslash}displaystyle (X,Y)\}
   is to the product of the marginal distributions of 
  
    
      
        X
      
    
    \{{\textbackslash}displaystyle X\}
   and 
  
    
      
        Y
      
    
    \{{\textbackslash}displaystyle Y\}
  . MI is the expected value of the pointwise mutual information (PMI).},
	language = {en},
	urldate = {2019-04-26},
	journal = {Wikipedia},
	month = apr,
	year = {2019},
	note = {Page Version ID: 892560218},
	file = {Snapshot:/Users/adrian/Zotero/storage/22PMAS6S/index.html:text/html}
}

@article{nott_estimation_2002-1,
	title = {Estimation of nonstationary spatial covariance structure},
	volume = {89},
	issn = {0006-3444, 1464-3510},
	url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/89.4.819},
	doi = {10.1093/biomet/89.4.819},
	abstract = {We introduce a method for estimating nonstationary spatial covariance structure from space-time data and apply the method to an analysis of Sydney wind patterns. Our method constructs a process honouring a given spatial covariance matrix at observing stations and uses one or more stationary processes to describe conditional behaviour given observing site values. The stationary processes give a localised description of the spatial covariance structure. The method is computationally attractive, and can be extended to the assessment of covariance for multivariate processes. The technique is illustrated for data describing the east-west component of Sydney winds. For this example, our own methods are contrasted with a geometrically appealing though computationally intensive technique which describes spatial correlation via an isotropic process and a deformation of the geographical space.},
	language = {en},
	number = {4},
	urldate = {2019-05-01},
	journal = {Biometrika},
	author = {Nott, D. J. and Dunsmuir, W. T. M.},
	month = dec,
	year = {2002},
	pages = {819--829},
	file = {Nott and Dunsmuir - 2002 - Estimation of nonstationary spatial covariance str.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Nott and Dunsmuir - 2002 - Estimation of nonstationary spatial covariance str.pdf:application/pdf}
}

@incollection{bordeaux_submodular_2013,
	address = {Cambridge},
	title = {Submodular {Function} {Maximization}},
	isbn = {978-1-139-17780-1},
	url = {https://www.cambridge.org/core/product/identifier/CBO9781139177801A031/type/book_part},
	language = {en},
	urldate = {2019-05-02},
	booktitle = {Tractability},
	publisher = {Cambridge University Press},
	author = {Krause, Andreas and Golovin, Daniel},
	editor = {Bordeaux, Lucas and Hamadi, Youssef and Kohli, Pushmeet and Mateescu, Robert},
	year = {2013},
	doi = {10.1017/CBO9781139177801.004},
	pages = {71--104},
	file = {Krause and Golovin - 2013 - Submodular Function Maximization.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Submodularity/Krause and Golovin - 2013 - Submodular Function Maximization.pdf:application/pdf}
}

@article{nemhauser_analysis_1978,
	title = {An analysis of approximations for maximizing submodular set functions—{I}},
	volume = {14},
	issn = {0025-5610, 1436-4646},
	url = {http://link.springer.com/10.1007/BF01588971},
	doi = {10.1007/BF01588971},
	abstract = {Let N be a finite set and z be a real-valued function defined on the set of subsets of N that satisfies z ( S ) + z ( T ) {\textgreater} - z ( S U T ) + z ( S n T ) for all S, T in N. Such a function is called submodular. We consider the problem maXscN \{z(S): IS[ {\textless}-K, z(S) submodular\}. Several hard combinatorial optimization problems can be posed in this framework. For example, the problem of finding a maximum weight independent set in a matroid, when the elements of the matroid are colored and the elements of the independent set can have no more than K colors, is in this class. The uncapacitated location problem is a special case of this matroid optimization problem. We analyze greedy and local improvement heuristics and a linear programming relaxation for this problem. Our results are worst case bounds on the quality of the approximations. For example, when z(S) is nondecreasing and z(0) = 0, we.show that a "greedy" heuristic always produces a solution whose value is at least 1 - [ ( K - 1 ) / K ] K times the optimal value. This bound can be achieved for each K and has a limiting value of ( e - l)/e, where e is the base of the natural logarithm.},
	language = {en},
	number = {1},
	urldate = {2019-05-02},
	journal = {Mathematical Programming},
	author = {Nemhauser, G. L. and Wolsey, L. A. and Fisher, M. L.},
	month = dec,
	year = {1978},
	pages = {265--294},
	file = {Nemhauser et al. - 1978 - An analysis of approximations for maximizing submo.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Submodularity/Nemhauser et al. - 1978 - An analysis of approximations for maximizing submo.pdf:application/pdf}
}

@inproceedings{leskovec_cost-effective_2007,
	address = {San Jose, California, USA},
	title = {Cost-effective outbreak detection in networks},
	isbn = {978-1-59593-609-7},
	url = {http://portal.acm.org/citation.cfm?doid=1281192.1281239},
	doi = {10.1145/1281192.1281239},
	abstract = {Given a water distribution network, where should we place sensors to quickly detect contaminants? Or, which blogs should we read to avoid missing important stories? These seemingly diﬀerent problems share common structure: Outbreak detection can be modeled as selecting nodes (sensor locations, blogs) in a network, in order to detect the spreading of a virus or information as quickly as possible.},
	language = {en},
	urldate = {2019-05-02},
	booktitle = {Proceedings of the 13th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining  - {KDD} '07},
	publisher = {ACM Press},
	author = {Leskovec, Jure and Krause, Andreas and Guestrin, Carlos and Faloutsos, Christos and VanBriesen, Jeanne and Glance, Natalie},
	year = {2007},
	pages = {420},
	file = {Leskovec et al. - 2007 - Cost-effective outbreak detection in networks.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Submodularity/Other Applications/Leskovec et al. - 2007 - Cost-effective outbreak detection in networks.pdf:application/pdf}
}

@article{deisenroth_distributed_nodate,
	title = {Distributed {Gaussian} {Processes}},
	abstract = {To scale Gaussian processes (GPs) to large data sets we introduce the robust Bayesian Committee Machine (rBCM), a practical and scalable product-of-experts model for large-scale distributed GP regression. Unlike state-of-theart sparse GP approximations, the rBCM is conceptually simple and does not rely on inducing or variational parameters. The key idea is to recursively distribute computations to independent computational units and, subsequently, recombine them to form an overall result. Efﬁcient closed-form inference allows for straightforward parallelisation and distributed computations with a small memory footprint. The rBCM is independent of the computational graph and can be used on heterogeneous computing infrastructures, ranging from laptops to clusters. With sufﬁcient computing resources our distributed GP model can handle arbitrarily large data sets.},
	language = {en},
	author = {Deisenroth, Marc Peter and Ng, Jun Wei},
	pages = {10},
	file = {Deisenroth and Ng - Distributed Gaussian Processes.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Scalability/Deisenroth and Ng - Distributed Gaussian Processes.pdf:application/pdf}
}

@book{cressie_statistics_1991,
	address = {New York},
	series = {Wiley series in probability and mathematical statistics},
	title = {Statistics for spatial data},
	isbn = {978-0-471-84336-8},
	publisher = {Wiley},
	author = {Cressie, Noel A. C.},
	year = {1991},
	keywords = {Spatial analysis (Statistics)},
	file = {Cressie - 1991 - Statistics for spatial data.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Cressie - 1991 - Statistics for spatial data.pdf:application/pdf}
}

@misc{noauthor_stationary_2019,
	title = {Stationary process},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Stationary_process&oldid=887919881},
	abstract = {In mathematics and statistics, a stationary process (a.k.a. a strict/strictly stationary process or strong/strongly stationary process) is a stochastic process whose unconditional joint probability distribution does not change when shifted in time. Consequently, parameters such as mean and variance also do not change over time.
Since stationarity is an assumption underlying many statistical procedures used in time series analysis, non-stationary data is often transformed to become stationary. The most common cause of violation of stationarity is a trend in the mean, which can be due either to the presence of a unit root or of a deterministic trend. In the former case of a unit root, stochastic shocks have permanent effects, and the process is not mean-reverting. In the latter case of a deterministic trend, the process is called a trend stationary process, and stochastic shocks have only transitory effects after which the variable tends toward a deterministically evolving (non-constant) mean.
A trend stationary process is not strictly stationary, but can easily be transformed into a stationary process by removing the underlying trend, which is solely a function of time. Similarly, processes with one or more unit roots can be made stationary through differencing. An important type of non-stationary process that does not include a trend-like behavior is a cyclostationary process, which is a stochastic process that varies cyclically with time.
For many applications strict-sense stationarity is too restrictive. Other forms of stationarity such as wide-sense stationarity or N-th order stationarity are then employed. The definitions for different kinds of stationarity are not consistent among different authors (see Other terminology).},
	language = {en},
	urldate = {2019-05-02},
	journal = {Wikipedia},
	month = mar,
	year = {2019},
	note = {Page Version ID: 887919881},
	file = {Snapshot:/Users/adrian/Zotero/storage/GH5L2HT4/index.html:text/html}
}

@article{zhu_model_2019,
	title = {Model error correction in data assimilation by integrating neural networks},
	volume = {2},
	issn = {2096-0654},
	url = {https://ieeexplore.ieee.org/document/8665726/},
	doi = {10.26599/BDMA.2018.9020033},
	abstract = {In this paper, we suggest a new methodology which combines Neural Networks (NN) into Data Assimilation (DA). Focusing on the structural model uncertainty, we propose a framework for integration NN with the physical models by DA algorithms, to improve both the assimilation process and the forecasting results. The NNs are iteratively trained as observational data is updated. The main DA models used here are the Kalman ﬁlter and the variational approaches. The effectiveness of the proposed algorithm is validated by examples and by a sensitivity study.},
	language = {en},
	number = {2},
	urldate = {2019-05-06},
	journal = {Big Data Mining and Analytics},
	author = {Zhu, Jiangcheng and Hu, Shuang and Arcucci, Rossella and Xu, Chao and Zhu, Jihong and Guo, Yi-ke},
	month = jun,
	year = {2019},
	pages = {83--91},
	file = {Zhu et al. - 2019 - Model error correction in data assimilation by int.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Data Assimilation/Zhu et al. - 2019 - Model error correction in data assimilation by int.pdf:application/pdf}
}

@article{arcucci_effective_2018,
	title = {Effective variational data assimilation in air-pollution prediction},
	volume = {1},
	issn = {2096-0654},
	url = {https://ieeexplore.ieee.org/document/8400446/},
	doi = {10.26599/BDMA.2018.9020025},
	abstract = {Numerical simulations are widely used as a predictive tool to better understand complex air ﬂows and pollution transport on the scale of individual buildings, city blocks, and entire cities. To improve prediction for air ﬂows and pollution transport, we propose a Variational Data Assimilation (VarDA) model which assimilates data from sensors into the open-source, ﬁnite-element, ﬂuid dynamics model Fluidity. VarDA is based on the minimization of a function which estimates the discrepancy between numerical results and observations assuming that the two sources of information, forecast and observations, have errors that are adequately described by error covariance matrices. The conditioning of the numerical problem is dominated by the condition number of the background error covariance matrix which is ill-conditioned. In this paper, a preconditioned VarDA model is presented, it is based on a reduced background error covariance matrix. The Empirical Orthogonal Functions (EOFs) method is used to alleviate the computational cost and reduce the space dimension. Experimental results are provided assuming observed values provided by sensors from positions mainly located on roofs of buildings.},
	language = {en},
	number = {4},
	urldate = {2019-05-06},
	journal = {Big Data Mining and Analytics},
	author = {Arcucci, Rossella and Pain, Christopher and Guo, Yi-Ke},
	month = dec,
	year = {2018},
	pages = {297--307},
	file = {Arcucci et al. - 2018 - Effective variational data assimilation in air-pol.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Data Assimilation/Arcucci et al. - 2018 - Effective variational data assimilation in air-pol.pdf:application/pdf}
}

@incollection{monestiez_advances_2001,
	address = {Dordrecht},
	title = {Advances in {Modeling} and {Inference} for {Environmental} {Processes} with {Nonstationary} {Spatial} {Covariance}},
	volume = {11},
	isbn = {978-0-7923-7107-6 978-94-010-0810-5},
	url = {http://www.springerlink.com/index/10.1007/978-94-010-0810-5_2},
	urldate = {2019-05-16},
	booktitle = {{geoENV} {III} — {Geostatistics} for {Environmental} {Applications}},
	publisher = {Springer Netherlands},
	author = {Sampson, P. D. and Damian, D. and Guttorp, P.},
	editor = {Monestiez, Pascal and Allard, Denis and Froidevaux, Roland},
	year = {2001},
	doi = {10.1007/978-94-010-0810-5_2},
	pages = {17--32},
	file = {Sampson et al. - 2001 - Advances in Modeling and Inference for Environment.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Sampson et al. - 2001 - Advances in Modeling and Inference for Environment.pdf:application/pdf}
}

@incollection{guttorp_20_1994,
	title = {20 {Methods} for estimating heterogeneous spatial covariance functions with environmental applications},
	volume = {12},
	isbn = {978-0-444-89803-6},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169716105800227},
	abstract = {Estimation of spatial covariance is important to many statistical problems in the analysis of environmental monitoring data. In this Chapter we review several difrerent methods for spati.al covariance estimation from monitoring data, with emphasis on methods for heterogeneous models. We briefly describe some applications, and outline how these methods can be extended to space-time and multivariate models.},
	language = {en},
	urldate = {2019-05-16},
	booktitle = {Handbook of {Statistics}},
	publisher = {Elsevier},
	author = {Guttorp, Peter and Sampson, Paul D.},
	year = {1994},
	doi = {10.1016/S0169-7161(05)80022-7},
	pages = {661--689},
	file = {Guttorp and Sampson - 1994 - 20 Methods for estimating heterogeneous spatial co.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Guttorp and Sampson - 1994 - 20 Methods for estimating heterogeneous spatial co.pdf:application/pdf}
}

@article{liu_when_2018,
	title = {When {Gaussian} {Process} {Meets} {Big} {Data}: {A} {Review} of {Scalable} {GPs}},
	shorttitle = {When {Gaussian} {Process} {Meets} {Big} {Data}},
	url = {http://arxiv.org/abs/1807.01065},
	abstract = {The vast quantity of information brought by big data as well as the evolving computer hardware encourages success stories in the machine learning community. In the meanwhile, it poses challenges for the Gaussian process (GP) regression, a well-known non-parametric and interpretable Bayesian model, which suffers from cubic complexity to data size. To improve the scalability while retaining desirable prediction quality, a variety of scalable GPs have been presented. But they have not yet been comprehensively reviewed and analyzed in order to be well understood by both academia and industry. The review of scalable GPs in the GP community is timely and important due to the explosion of data size. To this end, this paper is devoted to the review on state-of-the-art scalable GPs involving two main categories: global approximations which distillate the entire data and local approximations which divide the data for subspace learning. Particularly, for global approximations, we mainly focus on sparse approximations comprising prior approximations which modify the prior but perform exact inference, posterior approximations which retain exact prior but perform approximate inference, and structured sparse approximations which exploit speciﬁc structures in kernel matrix; for local approximations, we highlight the mixture/product of experts that conducts model averaging from multiple local experts to boost predictions. To present a complete review, recent advances for improving the scalability and capability of scalable GPs are reviewed. Finally, the extensions and open issues regarding the implementation of scalable GPs in various scenarios are reviewed and discussed to inspire novel ideas for future research avenues.},
	language = {en},
	urldate = {2019-05-16},
	journal = {arXiv:1807.01065 [cs, stat]},
	author = {Liu, Haitao and Ong, Yew-Soon and Shen, Xiaobo and Cai, Jianfei},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.01065},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Liu et al. - 2018 - When Gaussian Process Meets Big Data A Review of .pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Scalability/Liu et al. - 2018 - When Gaussian Process Meets Big Data A Review of .pdf:application/pdf}
}

@article{sampson_nonparametric_1992,
	title = {Nonparametric {Estimation} of {Nonstationary} {Spatial} {Covariance} {Structure}},
	volume = {87},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1992.10475181},
	doi = {10.1080/01621459.1992.10475181},
	language = {en},
	number = {417},
	urldate = {2019-05-28},
	journal = {Journal of the American Statistical Association},
	author = {Sampson, Paul D. and Guttorp, Peter},
	month = mar,
	year = {1992},
	pages = {108--119},
	file = {Sampson and Guttorp - 1992 - Nonparametric Estimation of Nonstationary Spatial .pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Sampson and Guttorp - 1992 - Nonparametric Estimation of Nonstationary Spatial .pdf:application/pdf}
}

@article{vetter_efficient_2014,
	title = {Efficient {Approximation} of the {Spatial} {Covariance} {Function} for {Large} {Datasets}},
	abstract = {Linear mixed eﬀects models have been widely used in the spatial analysis of environmental processes. However, parameter estimation and spatial predictions involve the inversion and determinant of the n × n dimensional spatial covariance matrix of the data process, with n being the number of observations. Nowadays environmental variables are typically obtained through remote sensing and contain observations of the order of tens or hundreds of thousands on a single day, which quickly leads to bottlenecks in terms of computation speed and requirements in working memory. Therefore techniques for reducing the dimension of the problem are required. The present work analyzes approaches to approximate the spatial covariance function in a real dataset of remotely sensed carbon dioxide concentrations, obtained from the Atmospheric Infrared Sounder of NASA’s ”Aqua” satellite on the 1st of May 2009. In a cross-validation case study it is shown how ﬁxed rank kriging, stationary covariance tapering and the full-scale approximation are able to notably speed up calculations. However, the loss in predictive performance caused by the approximation strongly diﬀers. The best results were obtained for the full-scale approximation, which was able to overcome the individual weaknesses of the ﬁxed rank kriging and the covariance tapering.},
	language = {en},
	author = {Vetter, Patrick and Schmid, Wolfgang},
	month = aug,
	year = {2014},
	pages = {36},
	file = {Vetter and Schmid - 2014 - Efficient Approximation of the Spatial Covariance .pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Vetter and Schmid - 2014 - Efficient Approximation of the Spatial Covariance .pdf:application/pdf}
}

@article{pourahmadi_covariance_2011,
	title = {Covariance {Estimation}: {The} {GLM} and {Regularization} {Perspectives}},
	volume = {26},
	issn = {0883-4237},
	shorttitle = {Covariance {Estimation}},
	url = {http://arxiv.org/abs/1202.1661},
	doi = {10.1214/11-STS358},
	abstract = {Finding an unconstrained and statistically interpretable reparameterization of a covariance matrix is still an open problem in statistics. Its solution is of central importance in covariance estimation, particularly in the recent high-dimensional data environment where enforcing the positive-deﬁniteness constraint could be computationally expensive. We provide a survey of the progress made in modeling covariance matrices from two relatively complementary perspectives: (1) generalized linear models (GLM) or parsimony and use of covariates in low dimensions, and (2) regularization or sparsity for high-dimensional data. An emerging, unifying and powerful trend in both perspectives is that of reducing a covariance estimation problem to that of estimating a sequence of regression problems. We point out several instances of the regression-based formulation. A notable case is in sparse estimation of a precision matrix or a Gaussian graphical model leading to the fast graphical LASSO algorithm. Some advantages and limitations of the regression-based Cholesky decomposition relative to the classical spectral (eigenvalue) and variance-correlation decompositions are highlighted. The former provides an unconstrained and statistically interpretable reparameterization, and guarantees the positive-deﬁniteness of the estimated covariance matrix. It reduces the unintuitive task of covariance estimation to that of modeling a sequence of regressions at the cost of imposing an a priori order among the variables. Elementwise regularization of the sample covariance matrix such as banding, tapering and thresholding has desirable asymptotic properties and the sparse estimated covariance matrix is positive deﬁnite with probability tending to one for large samples and dimensions.},
	language = {en},
	number = {3},
	urldate = {2019-05-30},
	journal = {Statistical Science},
	author = {Pourahmadi, Mohsen},
	month = aug,
	year = {2011},
	note = {arXiv: 1202.1661},
	keywords = {Statistics - Methodology},
	pages = {369--387},
	file = {Pourahmadi - 2011 - Covariance Estimation The GLM and Regularization .pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Pourahmadi - 2011 - Covariance Estimation The GLM and Regularization .pdf:application/pdf}
}

@article{furrer_covariance_2006,
	title = {Covariance {Tapering} for {Interpolation} of {Large} {Spatial} {Datasets}},
	volume = {15},
	issn = {1061-8600, 1537-2715},
	url = {http://www.tandfonline.com/doi/abs/10.1198/106186006X132178},
	doi = {10.1198/106186006X132178},
	language = {en},
	number = {3},
	urldate = {2019-05-30},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Furrer, Reinhard and Genton, Marc G and Nychka, Douglas},
	month = sep,
	year = {2006},
	pages = {502--523},
	file = {Furrer et al. - 2006 - Covariance Tapering for Interpolation of Large Spa.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Furrer et al. - 2006 - Covariance Tapering for Interpolation of Large Spa.pdf:application/pdf}
}

@article{sun_geostatistics_2012,
	title = {Geostatistics for {Large} {Datasets}},
	abstract = {We review various approaches for the geostatistical analysis of large datasets. First, we consider covariance structures that yield computational simpliﬁcations in geostatistics and brieﬂy discuss how to test the suitability of such structures. Second, we describe the use of covariance tapering for both estimation and kriging purposes. Third, we consider likelihood approximations in both spatial and spectral domains. Fourth, we explore methods based on latent processes, such as Gaussian predictive processes and ﬁxed rank kriging. Fifth, we describe methods based on Gaussian Markov random ﬁeld approximations. Finally, we discuss multivariate extensions and open problems in this area.},
	language = {en},
	author = {Sun, Ying and Li, Bo and Genton, Marc G},
	year = {2012},
	pages = {23},
	file = {Sun et al. - 2012 - Geostatistics for Large Datasets.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Sun et al. - 2012 - Geostatistics for Large Datasets.pdf:application/pdf}
}

@article{cressie_fixed_2008,
	title = {Fixed rank kriging for very large spatial data sets: {Fixed} {Rank} {Kriging}},
	volume = {70},
	issn = {13697412},
	shorttitle = {Fixed rank kriging for very large spatial data sets},
	url = {http://doi.wiley.com/10.1111/j.1467-9868.2007.00633.x},
	doi = {10.1111/j.1467-9868.2007.00633.x},
	abstract = {Spatial statistics forvery largespatial data sets ischallenging. The size of the data set, n, causes problems incomputing optimal spatial predictors such as kriging, since itscomputa tional cost isof order A73.Inaddition, a large data set isoften defined on a large spatial domain, so the spatial process of interest typically exhibits non-stationary behaviour over that domain. A flexible familyof non-stationary covariance functions isdefined by using a set of basis functions that is fixed innumber, which leads to a spatial prediction method thatwe call fixed rankkriging. Specifically, fixed rankkriging iskrigingwithin this class of non-stationary covariance functions. It relies on computational simplifications when n is very large, for obtaining the spatial best linearunbiased predictor and itsmean-squared prediction error fora hidden spatial process. A method based on minimizing a weighted Frobenius norm yields best estimators of the covari ance function parameters, which are then substituted into the fixed rank kriging equations. The new methodology isapplied to a very large data set of total column ozone data, observed over the entire globe, where n isof the order of hundreds of thousands.},
	language = {en},
	number = {1},
	urldate = {2019-05-30},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Cressie, Noel and Johannesson, Gardar},
	month = jan,
	year = {2008},
	pages = {209--226},
	file = {Cressie and Johannesson - 2008 - Fixed rank kriging for very large spatial data set.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Cressie and Johannesson - 2008 - Fixed rank kriging for very large spatial data set.pdf:application/pdf}
}

@article{sang_full_2012,
	title = {A full scale approximation of covariance functions for large spatial data sets: {Approximation} of {Covariance} {Functions} for {Large} {Data} {Sets}},
	volume = {74},
	issn = {13697412},
	shorttitle = {A full scale approximation of covariance functions for large spatial data sets},
	url = {http://doi.wiley.com/10.1111/j.1467-9868.2011.01007.x},
	doi = {10.1111/j.1467-9868.2011.01007.x},
	abstract = {Gaussian process models have been widely used in spatial statistics but face tremendous computational challenges for very large data sets. The model ﬁtting and spatial prediction of such models typically require O(n3) operations for a data set of size n. Various approximations of the covariance functions have been introduced to reduce the computational cost. However, most existing approximations can not simultaneously capture both the large and small scale spatial dependence. A new approximation scheme is developed in this paper to provide a high quality approximation to the covariance function at both the large and small spatial scales. The new approximation is the summation of two parts: a reduced rank covariance and a compactly supported covariance obtained by tapering the covariance of the residual of the reduced rank approximation. While the former part mainly captures the large scale spatial variation, the latter part captures the small scale, local variation that is unexplained by the former part. By combining the reduced rank representation and sparse matrix techniques, our approach allows for efﬁcient computation for maximum likelihood estimation, spatial prediction and Bayesian inference. We illustrate the new approach with simulated and real data sets.},
	language = {en},
	number = {1},
	urldate = {2019-05-30},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Sang, Huiyan and Huang, Jianhua Z.},
	month = jan,
	year = {2012},
	pages = {111--132},
	file = {Sang and Huang - 2012 - A full scale approximation of covariance functions.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Sang and Huang - 2012 - A full scale approximation of covariance functions.pdf:application/pdf}
}

@article{rothman_positive_2012,
	title = {Positive definite estimators of large covariance matrices},
	volume = {99},
	issn = {0006-3444, 1464-3510},
	url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/ass025},
	doi = {10.1093/biomet/ass025},
	abstract = {Using convex optimization, we construct a sparse estimator of the covariance matrix that is positive definite and performs well in high-dimensional settings. A lasso-type penalty is used to encourage sparsity and a logarithmic barrier function is used to enforce positive definiteness. Consistency and convergence rate bounds are established as both the number of variables and sample size diverge. An efficient computational algorithm is developed and the merits of the approach are illustrated with simulations and a speech signal classification example.},
	language = {en},
	number = {3},
	urldate = {2019-05-30},
	journal = {Biometrika},
	author = {Rothman, A. J.},
	month = sep,
	year = {2012},
	pages = {733--740},
	file = {Rothman - 2012 - Positive definite estimators of large covariance m.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Rothman - 2012 - Positive definite estimators of large covariance m.pdf:application/pdf}
}

@article{rothman_sparse_2008,
	title = {Sparse permutation invariant covariance estimation},
	volume = {2},
	issn = {1935-7524},
	url = {http://projecteuclid.org/euclid.ejs/1214491853},
	doi = {10.1214/08-EJS176},
	abstract = {The paper proposes a method for constructing a sparse estimator for the inverse covariance (concentration) matrix in high-dimensional settings. The estimator uses a penalized normal likelihood approach and forces sparsity by using a lasso-type penalty. We establish a rate of convergence in the Frobenius norm as both data dimension p and sample size n are allowed to grow, and show that the rate depends explicitly on how sparse the true concentration matrix is. We also show that a correlationbased version of the method exhibits better rates in the operator norm. We also derive a fast iterative algorithm for computing the estimator, which relies on the popular Cholesky decomposition of the inverse but produces a permutation-invariant estimator. The method is compared to other estimators on simulated data and on a real data example of tumor tissue classiﬁcation using gene expression data.},
	language = {en},
	number = {0},
	urldate = {2019-05-30},
	journal = {Electronic Journal of Statistics},
	author = {Rothman, Adam J. and Bickel, Peter J. and Levina, Elizaveta and Zhu, Ji},
	year = {2008},
	pages = {494--515},
	file = {Rothman et al. - 2008 - Sparse permutation invariant covariance estimation.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Rothman et al. - 2008 - Sparse permutation invariant covariance estimation.pdf:application/pdf}
}

@article{bailey_multiple_2019,
	title = {A multiple testing approach to the regularisation of large sample correlation matrices},
	volume = {208},
	issn = {03044076},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304407618302008},
	doi = {10.1016/j.jeconom.2018.10.006},
	abstract = {This paper proposes a regularisation method for the estimation of large covariance matrices that uses insights from the multiple testing (MT ) literature. The approach tests the statistical significance of individual pair-wise correlations and sets to zero those elements that are not statistically significant, taking account of the multiple testing nature of the problem. The effective p-values of the tests are set as a decreasing function of N (the cross section dimension), the rate of which is governed by the nature of dependence of the underlying observations, and the relative expansion rates of N and T (the time dimension). In this respect, the method specifies the appropriate thresholding parameter to be used under Gaussian and non-Gaussian settings. The MT estimator of the sample correlation matrix is shown to be consistent in the spectral and Frobenius norms, and in terms of support recovery, so long as the true covariance matrix is sparse. The performance of the proposed MT estimator is compared to a number of other estimators in the literature using Monte Carlo experiments. It is shown that the MT estimator performs well and tends to outperform the other estimators, particularly when N is larger than T .},
	language = {en},
	number = {2},
	urldate = {2019-05-30},
	journal = {Journal of Econometrics},
	author = {Bailey, Natalia and Pesaran, M. Hashem and Smith, L. Vanessa},
	month = feb,
	year = {2019},
	pages = {507--534},
	file = {Bailey et al. - 2019 - A multiple testing approach to the regularisation .pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Bailey et al. - 2019 - A multiple testing approach to the regularisation .pdf:application/pdf}
}

@article{cui_sparse_2016,
	title = {Sparse estimation of high-dimensional correlation matrices},
	volume = {93},
	issn = {01679473},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S016794731400293X},
	doi = {10.1016/j.csda.2014.10.001},
	abstract = {Several attempts to estimate covariance matrices with sparsity constraints have been made. A convex optimization formulation for estimating correlation matrices as opposed to covariance matrices is proposed. An efficient accelerated proximal gradient algorithm is developed, and it is shown that this method gives a faster rate of convergence. An adaptive version of this approach is also discussed. Simulation results and an analysis of a cardiovascular microarray confirm its performance and usefulness.},
	language = {en},
	urldate = {2019-05-30},
	journal = {Computational Statistics \& Data Analysis},
	author = {Cui, Ying and Leng, Chenlei and Sun, Defeng},
	month = jan,
	year = {2016},
	pages = {390--403},
	file = {Cui et al. - 2016 - Sparse estimation of high-dimensional correlation .pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Cui et al. - 2016 - Sparse estimation of high-dimensional correlation .pdf:application/pdf}
}

@article{maurya_joint_2014,
	title = {A joint convex penalty for inverse covariance matrix estimation},
	volume = {75},
	issn = {01679473},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167947314000267},
	doi = {10.1016/j.csda.2014.01.015},
	abstract = {The paper proposes a joint convex penalty for estimating the Gaussian inverse covariance matrix. A proximal gradient method is developed to solve the resulting optimization problem with more than one penalty constraints. The analysis shows that imposing a single constraint is not enough and the estimator can be improved by a trade-off between two convex penalties. The developed framework can be extended to solve wide arrays of constrained convex optimization problems. A simulation study is carried out to compare the performance of the proposed method to graphical lasso and the SPICE estimate of the inverse covariance matrix.},
	language = {en},
	urldate = {2019-05-30},
	journal = {Computational Statistics \& Data Analysis},
	author = {Maurya, Ashwini},
	month = jul,
	year = {2014},
	pages = {15--27},
	file = {Maurya - 2014 - A joint convex penalty for inverse covariance matr.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Maurya - 2014 - A joint convex penalty for inverse covariance matr.pdf:application/pdf}
}

@article{cressie_fitting_1985,
	title = {Fitting variogram models by weighted least squares},
	volume = {17},
	issn = {0020-5958, 1573-8868},
	url = {http://link.springer.com/10.1007/BF01032109},
	doi = {10.1007/BF01032109},
	abstract = {The method of weighted least squares is shown to be an appropriate way of fitting variogram models. The weighting scheme automatically gives most weight to early lags and downweights those lags with a small number o f pairs. Although weights are derived assuming the data are Gaussian (normal), they are shown to be still appropriate in the setting where data are a (smooth) transform o f the Gaussian case. The method o f (iterated) generalized least squares, which takes into account correlation between variogram estimators at different lags, offer more statistical efficiency at the price of more complexity. Weighted least squares for the robust estimator, based on square root differences, is less o f a compromise.},
	language = {en},
	number = {5},
	urldate = {2019-06-01},
	journal = {Journal of the International Association for Mathematical Geology},
	author = {Cressie, Noel},
	month = jul,
	year = {1985},
	pages = {563--586},
	file = {Cressie - 1985 - Fitting variogram models by weighted least squares.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Cressie - 1985 - Fitting variogram models by weighted least squares.pdf:application/pdf}
}

@book{wikle_spatio-temporal_2019,
	address = {Boca Raton, Florida : CRC Press, [2019]},
	edition = {1},
	title = {Spatio-{Temporal} {Statistics} with {R}},
	isbn = {978-1-351-76972-3},
	url = {https://www.taylorfrancis.com/books/9780429649783},
	language = {en},
	urldate = {2019-06-01},
	publisher = {Chapman and Hall/CRC},
	author = {Wikle, Christopher K. and Zammit-Mangion, Andrew and Cressie, Noel},
	month = feb,
	year = {2019},
	doi = {10.1201/9781351769723},
	file = {Wikle et al. - 2019 - Spatio-Temporal Statistics with R.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Wikle et al. - 2019 - Spatio-Temporal Statistics with R.pdf:application/pdf}
}

@article{haas_kriging_1990,
	title = {Kriging and automated variogram modeling within a moving window},
	volume = {24},
	issn = {09601686},
	url = {https://linkinghub.elsevier.com/retrieve/pii/096016869090508K},
	doi = {10.1016/0960-1686(90)90508-K},
	abstract = {A spatial estimation procedure based on ordinary kriging is described and evaluated which consists of using only samplingsitescontained within a movingwindowcentered at the estimate location for modeling the covariance structure and constructing the kriging equations. The moving window, by depending on local data only to estimate the spatial covariance structure and calculate the estimate, is less affected by spatial trend in the data than conventional krigingapproaches and implicitlymodels covariance nonstationarity. The window's covariance structure is estimated by automatically fitting a spherical variogram model to the unbiased estimatesof semi-variancecalculatedat severallags.The automatic fit uses nonlinear least squares regression constrained by the nugget parameter being nonnegative.},
	language = {en},
	number = {7},
	urldate = {2019-06-01},
	journal = {Atmospheric Environment. Part A. General Topics},
	author = {Haas, Timothy C.},
	month = jan,
	year = {1990},
	pages = {1759--1769},
	file = {Haas - 1990 - Kriging and automated variogram modeling within a .pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Haas - 1990 - Kriging and automated variogram modeling within a .pdf:application/pdf}
}

@misc{magic_magic_nodate,
	title = {{MAGIC}, {Envisaging}  a world with greener cities},
	url = {http://magic-air.uk/attorneys.html},
	abstract = {Managing Air in Green Inner Cities- MAGIC - is an EPSRC collaborative project between The University of Cambridge, University of Surrey and Imperial College London examining how we can create cities with no air pollution or urban-heat islands by 2050, to help combat climate change.},
	language = {en},
	urldate = {2019-06-03},
	journal = {Magic Air},
	author = {MAGIC},
	file = {Snapshot:/Users/adrian/Zotero/storage/6IANT66M/attorneys.html:text/html}
}

@book{rasmussen_gaussian_2006,
	address = {Cambridge, Mass},
	series = {Adaptive computation and machine learning},
	title = {Gaussian processes for machine learning},
	isbn = {978-0-262-18253-9},
	language = {en},
	publisher = {MIT Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	year = {2006},
	note = {OCLC: ocm61285753},
	keywords = {Data processing, Gaussian processes, Machine learning, Mathematical models},
	file = {RW.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/RW.pdf:application/pdf}
}