\documentclass[12pt,twoside]{report}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Definitions for the title page
% Edit these to provide the correct information
% e.g. \newcommand{\reportauthor}{Timothy Kimber}

\newcommand{\reporttitle}{ Gaussian Processes for \\ Optimal Sensor Position \\[0.5cm] { \Large  Background \& Progress Report}}
\newcommand{\reportauthor}{Adrian \textsc{Löwenstein} \\ \phantom{blank author} }
\newcommand{\supervisor}{Rossella \textsc{Arcucci} \\ Miguel \textsc{Molina-Solana}}

\newcommand{\degreetype}{Computing (Machine Learning)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% load some definitions and default packages
\input{includes}

% load some macros
\input{notation}

\date{June 2019}

\begin{document}

% load title page
\input{titlepage}


% page numbering etc.
\pagenumbering{roman}
\clearpage{\pagestyle{empty}\cleardoublepage}
\setcounter{page}{1}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{abstract}
%Your abstract.
%\end{abstract}

\cleardoublepage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section*{Acknowledgments}
%Comment this out if not needed.

\clearpage{\pagestyle{empty}\cleardoublepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%--- table of contents
\fancyhead[RE,LO]{\sffamily {Table of Contents}}
\tableofcontents 


\clearpage{\pagestyle{empty}\cleardoublepage}
\pagenumbering{arabic}
\setcounter{page}{1}
\fancyhead[LE,RO]{\slshape \rightmark}
\fancyhead[LO,RE]{\slshape \leftmark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Project Proposal}


\paragraph{Project Proposal:}
Gaussian processes (GP) have been widely used since the 1970’s in the fields of geostatistics and meteorology. Current applications are in diverse fields including sensor placement
In this project, we propose the employment of a GP model to calculate the optimal spatial positioning of sensors to study and collect air pollution data in big cities. We will then validate the results by means of a data assimilation software with the data at the proposed positions.

\paragraph{Dataset:} London South Bank University (LSBU) air pollution data (velocity, tracer)



 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background}

In this chapter we will cover the literature and the theory that will be used throughout the project. First we will review the context of the project and how it fits into the \textbf{MAGIC} project. Then our focus goes to the definition of \textbf{Gaussian Processes} (GP) and how they are used in the context of geospatial data. Furthermore, the use of GP relies heavily on \textbf{Covariance} matrixes which needs to be estimated. Those tools enable us to create \textbf{optimisation} algorithms for the position of sensors. Finally we will quickly explore the concepts of \textbf{Data Assimilation} (DA) that will be used to validate the results of the optimisation. 

\section{The MAGIC Project}

This work is done in the context of the \textbf{Managing Air for Green Inner Cities} project. This is a multidisciplinary project and has for objective to find solutions to the pollution and heating phenomenons in cities. Traditionally, urban environmental control relies on polluting and energy consuming heating, ventilation and cooling (HVAC) systems. The usage of the systems increases the heat and the pollution levels, inducing an increased need for the HVAC. The MAGIC project aims at breaking this vicious circle and has for objective to provide tools to make possible the design of cities acting as a natural HVAC system. \\


This has been extensively discussed by  \cite{song_natural_2018}.  For this purpose, integrated management and decision-support system is under development. It includes a variety of simulations for pollutants and temperature at different scales; a set of mathematical tools to allow fast computation in the context of real-time analysis; and cost-benefit models to asses the viability of the planning options and decisions. \\

As explained by \cite{song_natural_2018}, the test site which has been selected to conduct the study is a real urban area located in London South Bank University (LSBU) in  Elephant and Castle, London. In order to investigate the effect of ventilation on the cities problem, researchers in the MAGIC project have created simulations and experiments both in outdoor and indoor conditions, on the test site. They used wind tunnel experiments and computational fluid dynamics (CFD) to simulate the outdoor environment. Further works include the development of reduced-order modelling (ROM) in order to make faster the simulations while keeping a high level of accuracy \citep{arcucci_effective_2018}. \\

Another key research direction in the use Data Assimilation (DA) and more specifically Variational DA (VarDA) for assimilating measured data in real time and allowing better prediction of the model in the near future \citep{arcucci_effective_2018}. The further use of those method would be the optimisation of the position of the sensors which provide information for the VarDA.


\section{Gaussian Processes}

In this chapter we will review Gaussian Processes (GP) which are probabilistic models for spatial predictions based on observations assumption. \\ 

As explained by \citet[p.~29]{rasmussen_gaussian_2006}, the history of Gaussian Processes goes back at least as far as the 1940s. A lot of usages were developed in various fields. Notably for predictions in spatial statistics \citep{cressie_statistics_1991}.  Applied in particular in Geostatistics with methods known as \textit{kringing}, and in Meteorology. Gradually GP started to be be used in more general cases for regression. Nowadays it is often used in the context of Machine Learning. \\


For our problem we will be modeling the data of the sensor with GPs.
%As for sensor optimisation, we will follow the approach that was developed by \citet{krause_near-optimal_2008}. This method relies on GP for finding a near-optimal solution to the problem of placing sensors. 

\subsection{Sensor Data Modelling}

\subsubsection{Multivariate Gaussian Distribution}

GP will serve as a basis tool in our project. In the space we are monitoring we have a certain number of sensors measuring a certain quantity, such as temperature, pressure, speed of the wind or the concentration of a pollutant at a given position. We assume that the measured quantity  has a \textit{multivariate Gaussian joint distribution} between each point of the space. The associated random variable is $\X_\V$ for the set of locations $\V$ we would have the following distribution : $P(\X_\V = \vec{x}_\V) \sim \mathcal{N}(\mu_\V, K_{\V\V}) $, or explicitly : 


\begin{equation}
	P(\X_\V = \vec{x}_\V) \frac{1}{(2\pi)^{n/2} |K_{\V\V}|} \exp^{-\frac{1}{2}(\vec{x}_\V - \mu_\V)^T K_{\V\V}^{-1} (\vec{x}_\V - \mu_\V)}
\end{equation}


\subsubsection{Prediction with Gaussian Processes}
Let us still consider that we have the set of locations $\V$ and a set of sensors $\A$. In order to predict the quantity at positions were we have no sensors ($\V \backslash \A $) we can use a Gaussian Process. This GP is associated with a \textbf{mean function} $\mathcal{M}(\cdot)$ and a symmetric positive-definite \textbf{kernel function} $\mathcal{K}(\cdot,\cdot)$. We will denote the mean function values for a set of positions $\A$ by $\mu_\A$ and the kernel function values, or covariance matrix, between those points by $K_\A$. More detailed definitions are available in \citet[p.~13-16]{rasmussen_gaussian_2006}. \\

For a set of observations $\vec{x}_\A$ at positions $\A$ we can express for a finite set of other positions $\V \backslash \A $ the conditional distribution of those values. This means that we are able, for each point $y \in \V \backslash \A $, to predict the mean and the variance of $\vec{x}_y$. Using conditional distribution for the Multivariate Gaussian Distribution \citep[p.~193]{deisenroth_mathematics_2018}, we are able to express the following : 
\begin{align}
	P(\X_y | \vec{x}_\A ) &= \mathcal{N}(\mu_{y | \A}, K_{y | \A}) \\
	\mu_{y | \A} &= \mu_y + K_{y\A} K_{\A\A}^{-1} (y - \mu_y)\\ 
	K_{y | \A} &=  K_{yy} - K_{y\A} K_{\A\A}^{-1} K_{\A y} \label{equ:covGP}
\end{align}

An important point to notice is that the predicted covariance for the point y is not dependent of the values measured at $\A$, this is really useful because if allows us to define the uncertainty at $y$ without using only the covariance. \\

\subsection{Scalable Gaussian Processes} 

The biggest weakness of Standard GPs is their complexity. For $p$ training points, the algorithm requires the inversion of a $p \times p$ covariance matrix $K_{pp}$. \citet{liu_when_2018} gives an extensive review of all methods used to make the GPs more scalable. \\

\citet{liu_when_2018} provides a survey proposing several startegies, both global and local. 

%One simple approach is to  covariance matrix. In order to make it operation more stable numerically, we will use the matrix inversion algorithm proposed by \cite[p.~19]{rasmussen_gaussian_2006}. It use Cholesky factorisation of the covariance matrix to compute the covariance estimate of equation \ref{equ:covGP}. 

\textbf{Note :} This section needs a lot a further development, in order to find an implement the best possible approach. 

\subsection{Covariance Matrix Estimation} \label{sec:cov_est}

We have seen how GPs are defined and made more scalable. In order to have good results we need to have a good estimate of the covariance matrix between the points of our space. \\



%Many application require the covariance to be estimated because of the lack of measures. Very often a limited number of sensors has been in position and the key challenge is to interpolate the covariance. talk of the different methods\\

In our specific case, we already have at our disposal a very dense network of measurement. With more than $100'000$ different locations we don't need to explore the space outside of those points. Unfortunately the sample covariance is a very bad estimator of the true covariance in high dimensional settings such as ours \citep{pourahmadi_covariance_2011}. \\


We will see the properties that our covariance to accurately model our space, before discussing the best ways of estimating the covariance matrix estimation.


\subsubsection{Properties of Covariance}

By definition a covariance matrix must be \textbf{positive-definite} and \textbf{symmetrical} \citep[p.~80]{rasmussen_gaussian_2006}. \\

A covariance function between two inputs $x$ and $x'$ is \textbf{stationary}   when it is invariant to translation. Thus, when it is a function of $x' - x$. \\
In a covariance function that is \textbf{isotropic}, we remove the dependance of the direction and, it become a function of the distance between the two points :  $|x' - x|$. The isotropy of the covariance function is a stronger assumption than  \\ 

In our problem we can't assume that the process is stationary nor isotropic. Our space is 3-dimensional and not homogeneous. The presence of the buildings and other obstacles that a likely to make environment variables less smooth \citep{paciorek_nonstationary_2004}. Also it has been shown by \citet{krause_near-optimal_2008} that non-stationary covariance matrixes give better results than stationary or isotropic. This makes us choose a non-stationary covariance matrix for the  GPs of our problem.


\subsubsection{Covariance Estimation}

We consider the process $Y$ in $p$ different locations at $T$ sampling times. $\vec{Y}_t = (Y_{1t}, \dots, Y_{pt})$, with $t = 1 \dots T$. We want to estimate the covariance matrix $\Sigma$. \\

\paragraph{Sample Covariance}

The simplest estimator of the covariance matrix is the \textbf{sample covariance matrix} $\vec{S}$ directly computed from the captured data. 

\begin{equation}
	\vec{S} = \frac{1}{T -1} \sum_{t=1}^T (\vec{Y}_t - \bar{\vec{Y}})\cdot (\vec{Y}_t - \bar{\vec{Y}}_t)^T, \quad \bar{\vec{Y}} = \frac{1}{T} \sum_{t=1}^T \vec{Y}_t
\end{equation}

Unfortunately this  covariance is singular when $p >> T$ \citep{fan_overview_2015}, and the accumulation of errors due to the number of parameters to estimate. One additional assumption that we need to make for this covariance is its the sparsity of the covariance which is needed to reduce the number of parameters and is often realistic in practice. 

\paragraph{Estimating a Spatial Covariance} 

\cite{nott_estimation_2002-1} proposed to fit local variograms in order to approaximate the covariance matrix. It is a method that was developped in \cite{cressie_statistics_1991}. \\

An other method is the use of spatial deformation of kernel to orther of obtain non-stationarity \citep{sampson_nonparametric_1992}. \\

Several different approaches are developed in the literature, notably the works of \citep{fan_overview_2015} and \cite{guttorp_20_1994} which it more focused on spatial data. 

\textbf{Note :} This section needs a lot a further development, in order to find an implement the best possible approach. 

\section{Sensor Position Optimisation}

Now that we have modelled the relationship between the positions using GPS we can establish an algorithm that was developed by \citet{krause_near-optimal_2008}. The process of placing sensors in an optimal way is called in spatial statistics, \textit{sampling} or \textit{experimental design}. We want to find the optimal way place a number of $k$ sensors (indexed by $\A$) inside the set of possible sensor locations $\S$ . So that we have $\A \subseteq \S \subseteq \V$. \\


For the rest of this section we assume that we have at our disposal a good estimate a of the covariance matrix between each point. In practise this is not that obvious as we have seen in section \ref{sec:cov_est}. The following is valid for any covariance matrix that is symmetric and positive-definite. \\ 

First we will define how to characterise a good design in term of sensor placement. Then we will define the main optimisation algorithm and its improvements. \\ 

\subsection{Placement Criterion}

\subsubsection{Entropy Criterion}

Intuitively a good way of measuring uncertainty is the \textit{entropy}. By observing the conditional entropy of the location where no sensor was placed $\VA$, we can estimate the uncertainty remaining for those locations. We define the following conditional entropy of the un-instrumented location knowing the instrumented ones \citep[p.~16]{cover_elements_1991} :

\begin{align}
	H(\X_{\VA} | \X_\A) &= \mathbb{E}_{p(\vec{x}_{\VA},\vec{x}_\A)} \log p(\X_{\VA} | \X_\A) \\
	H(\X_{\VA} | \X_\A) &= - \sum_{\vec{x}_{\VA} \in \X_{\VA}}\sum_{\vec{x}_{\A} \in \X_{\A}} p(\vec{x}_{\VA},\vec{x}_\A) \log p(\vec{x}_{\VA}|\vec{x}_\A) \\
	H(\X_{\VA} | \X_\A) &= - \int p(\vec{x}_{\VA},\vec{x}_\A) \log p(\vec{x}_{\VA}|\vec{x}_\A) \: d\vec{x}_{\VA} \, d\vec{x}_\A 
\end{align}

For the specific case of the \textit{Multivariate Gaussian Distribution}, \citet{krause_near-optimal_2008} gives us the expression of the entropy of a point $y \in \VA$ conditioned by the set $\A$ in a closed form depending exclusively on the covariance between those elements : $K_{y | \A}$. Thus we have :  

\begin{equation}
	H(\X_{y} | \X_\A) = \frac{1}{2} \log K_{y | \A} + \frac{1}{2} \left(1 + \log 2\pi  \right) \label{equ:entGP}
\end{equation}

This formulation is extremely useful because we can directly use the expression of the covariance given by the \textit{Gaussian Process} expressed previously (\ref{equ:covGP}). \\


This conditional entropy can also be expressed using the \textit{chain rule}  \citep[p.~16]{cover_elements_1991} : 

\begin{align}
	H(\X_{\VA} | \X_\A) &= H(\X_{\VA} ,  \X_\A) -  H(\X_{\A}) \\
	&= H(\X_{\V}) -  H(\X_{\A}) 
\label{equ:chainentropy}
\end{align}

The optimal set of sensors $\A^*$ with size $|\A^*| = k$, is then defined for the minimum of this entropy. If we minimise this quantity we will reduce the uncertainty on the un-instrumented locations $\VA$ : 

\begin{align}
	\A^* &= {\arg\min}_{\A \subseteq \V : |\A| = k } \: H(\X_{\VA} | \X_\A) \\
	 &= {\arg\min}_{\A \subseteq \V : |\A| = k }\:  H(\X_\V) -  H(\X_{\A}) \\
	 &= {\arg\max}_{\A \subseteq \V : |\A| = k }\:  H(\X_{\A}) 
\end{align}


\subsubsection{Mutual Information Criterion}

The Entropy criterion provides an intuitive way to solve the problem, unfortunately, during experiments referenced by \citet{krause_near-optimal_2008}, it was noted that this criterion has a tendency to induce placement at the border of the space, and therefore wastes a lot of precious information. This is due to the fact that the entropy criterion is \textit{indirect} because it measures the uncertainty of the selected sensor position, instead of measuring the uncertainty of every other location of the space (which are the ones we are interested in). \\

An other criterion was proposed by \citet{caselton_optimal_1984} : the \textit{Mutual Information} (MI) Criterion. We try to maximise the mutual information between the set of selected sensors $\A$ and the rest of the space $\VA$. Using the definitions of MI provided by \citet[p.~19]{cover_elements_1991} : 

\begin{align}
	\A^* &= {\arg\max}_{\A \subseteq \V : |\A| = k } \: I(\X_{\VA} , \X_\A) \\
	&= {\arg\max}_{\A \subseteq \V : |\A| = k } \: H(\X_{\VA}) -  H(\X_{\VA} | \X_{\A})
\end{align}


Experimentally, \citet{krause_near-optimal_2008} explain that  the mutual information outperforms entropy placement optimisation. They also argue that this criterion relies heavily on the quality of the model $P(\X_\V)$ (i.e. how the covariance is modelled, see section \ref{sec:cov_est}) for giving good results. 

\subsection{Approximation Algorithm}
 
 As stated by \citet{krause_near-optimal_2008}, the problem is a \textit{NP-complete problem}. Therefore we present here an algorithm that is able to approximate in polynomial time the optimal solution, with a constant factor guarantee. \\
 
 Let us define the initial sensor set $\A_0 = \emptyset$. At each iteration we have a new sensor set : $\A$,  and for a point $y$, we define : $\bar{\A} = \V \backslash ( \A \cup y )$. We also have the set of positions that can be selected as sensors : $\S$,  and the associated set : $\U = \V \backslash \S $. \\
 
 
 The idea is to greedily add sensors until we reach the wanted number ($k$).   This greedy approach in enabled by the use the \textit{chain rule} of entropy.  Starting from $\A = \A_0$, at each iteration we add to $\A$ the point $y \in \S \backslash A $ which decreases the least the current MI : 
\begin{align}
	y* &= {\arg \max}_{y \in \S \backslash A } \: MI(\A \cup y, \bar{\A} ) - MI(\A, \VA) \\
	&= {\arg \max}_{y \in \S \backslash A } \: H(y | \A ) - H(y | \bar{\A} )
\end{align}

In our specific case with the Multivariate Gaussian Distribution, we can take the formulation presented in equation \ref{equ:entGP} and rewrite the objective to : 

\begin{align}
	y*  &= {\arg \max}_{y \in \S \backslash A } \: \frac{1}{2} \log K_{y | \A} - \frac{1}{2} \log K_{y | \bar{\A}} \\
	&= {\arg \max}_{y \in \S \backslash A } \: \log \left(\frac{K_{y | \A}}{K_{y | \bar{\A}}} \right) \\
	&= {\arg \max}_{y \in \S \backslash A } \: \frac{K_{y | \A}}{K_{y | \bar{\A}}}
\end{align}

Here we can use the GP that we have defined earlier in equation \ref{equ:covGP} to finally write the problem to solve at each iteration of the algorithm : 

\begin{equation}
	y*  = {\arg \max}_{y \in \S \backslash A } \: \frac{K_{yy} - K_{y\A} K_{\A\A}^{-1} K_{\A y} }{K_{yy} - K_{y\bar{\A}} K_{\bar{\A}\bar{\A}}^{-1} K_{\bar{\A} y} } \label{equ:updateGreedy}
\end{equation}

Then we update the set of sensors such that $\A = \A \cup y^*$, and then restart the process until $|\A| = k$. It is described in algorithm \ref{alg:greedy}  \\

\begin{algorithm}[h]
 \KwData{Covariance matrix $K_{\V\V}$ , $k$, $\V$}
 \KwResult{Sensor Selection $\A$}
 begin\;
 \For{$j \leftarrow 1$ \KwTo $k$}{
 	\For{$y \in \S \backslash \A $}{
 	$\delta_y \leftarrow \frac{K_{yy} - K_{y\A} K_{\A\A}^{-1} K_{\A y} }{K_{yy} - K_{y\bar{\A}} K_{\bar{\A}\bar{\A}}^{-1} K_{\bar{\A} y} }$
 	}
	$y^* \leftarrow {\arg \max}_{y \in \S \backslash A } \delta_y$ \\
	$\A \leftarrow \A \cup y^* $
 }
 \caption{Greedy Algorithm}
 \label{alg:greedy}
\end{algorithm}


This algorithm computes a solution that is very close to the optimal one if the discretisation of the space is small enough. The bound of solution obtained is approximately 63\% of the optimal solution. If the true optimal set is $\A^*$ and $\hat{\A}$ is the solution returned by the greedy algorithm, then \citet{krause_near-optimal_2008} proves, for a small $\epsilon >0 $, that : 
 \begin{equation}
	MI(\hat{\A}) \geq (1 - \frac{1}{e}) \cdot MI(\A^*) - k\epsilon
\end{equation}
To prove that they use the notion of \textit{submodularity} \citep{nemhauser_analysis_1978} applied to the $MI(\cdot)$ function. Intuitively this represents the notion of \textit{diminishing returns} :  adding a sensor to a small set of sensors has more benefits than adding a sensor to a large set of sensors. 


\subsection{Improvements over the Algorithm}


%\subsubsection{Scaling-up}

We have exposed the main greedy algorithm to solve that optimisation problem. \citet{krause_near-optimal_2008} explains that the complexity is a big issue for scaling this algorithm up. If we consider that ne number of locations in our space is $|\V| = n$, and that the number of sensors to place is $k$, the complexity of the main algorithm is $\mathcal{O}(kn^4)$. They propose two solutions to this issue : a \textit{lazy procedure} that cuts the complexity to  $\mathcal{O}(kn^3)$ and a \textit{local kernel} strategy that reduces it to $\mathcal{O}(kn)$

\subsubsection{Lazy Procedure} This procedure uses strategically the notion of \textit{submodularity} and \textit{priority queues}. We can describe it intuitively : When a sensor is selected $y^*$, the other nearby points will have de decreased $\delta_y$ and will therefore be less desirable. Therefore if we maintain a priority queue with the orderer values of $\delta_y$ at each step we will take the top values and update them to they current value successively. If the current value needs to be updated and the location is close to the previous optimum, it would have a small $\delta_y$ and be send back in the queue. If we meet a point which has been updated and is still a the top of the queue, it means that this point is our new optimum. This technique can be efficiently applied using \textbf{binary heaps}. This algorithm is described in algorithm \ref{alg:lazy}. \\ 


\begin{algorithm}[h]
 \KwData{Covariance matrix $K_{\V\V}$ , $k$, $\V$}
 \KwResult{Sensor Selection $\A$}
 initialisation\;
 $\A = \emptyset$ \\
 \lForEach{$y \in \S $}{$\delta_y \leftarrow + \infty$} 
 begin\;
 \For{$j \leftarrow 1$ \KwTo $k$}{
  	\lForEach{$y \in \S \backslash \A $}{$current_y \leftarrow $ False }
 	\While{True}{
 	$y^* \leftarrow {\arg \max}_{y \in \S \backslash A } \delta_y$ \\
	\lIf{$current_{y^*}$}{break}
 	$\delta_{y^*} $ is updated with \ref{equ:updateGreedy} \\
 	$current_y \leftarrow $ True
 	
 	}
	$\A \leftarrow \A \cup y^* $
 }
\caption{Lazy Algorithm}
\label{alg:lazy}
\end{algorithm}

\subsubsection{Local Kernels} This procedure takes advantage of the structure of the covariance matrix. For many GPs, correlation decreases exponentially with the distance between points. So, the idea here is to truncate the covariance matrix to points that are the most correlated. \\ 

If we want to compute the covariance between a point of interest $y$ and a set of points $\mathcal{B} \subset \V $ we have the original covariance matrix $K_{y\mathcal{B}}$. When we remove from the set $\mathcal{B}$, the set of elements  $x \in \S$ such that $|\K(y,x)| > \epsilon $ we define the new set  $\tilde{\mathcal{B}}_y = N(y,\epsilon)$ . This set is defined such as it contains less than $d$ locations : $|N(y,\epsilon)| \leq d $. The truncated covariance associated with this set is named : $\tilde{K}_{y\mathcal{B}}$. Finally, we define the approximate conditional entropy $\tilde{H}_\epsilon(y | \X) \simeq H(y | \X)$, computed with truncated covariance of the points of $N(y,\epsilon)$. The $\delta_y$ values are initialised by taking the difference between the true entropy and the truncated covariance, as :
\begin{align}
	\delta_y &= \tilde{H}_\epsilon(y | \A) - \tilde{H}_\epsilon(y | \bar{\A}) \\
			&= H(y) - \tilde{H}_\epsilon(y | \V \backslash y )
\end{align}
Or equivalently by keeping the previous notations: 
\begin{align}
	\delta_y &= \frac{K_{yy} }{K_{yy} - \tilde{K}_{y\V \backslash y} \tilde{K}_{\V \backslash y \V \backslash y}^{-1} \tilde{K}_{\V \backslash y y} } \label{equ:initLocal}
\end{align}


As for the other iterations (when $\A \neq \emptyset$), we have : 

\begin{align}
	\delta_y &= \tilde{H}_\epsilon(y | \A) - \tilde{H}_\epsilon(y | \bar{\A}) \\
	&= \frac{K_{yy} - \tilde{K}_{y\A} \tilde{K}_{\A\A}^{-1} \tilde{K}_{\A y} }{\tilde{K}_{yy} - \tilde{K}_{y\bar{\A}} \tilde{K}_{\bar{\A}\bar{\A}}^{-1} \tilde{K}_{\bar{\A} y} } \label{equ:updateLocal}
\end{align}


We explicitly define the steps  this in algorithm \ref{alg:local}. \\

\begin{algorithm}[h]
 \KwData{Covariance matrix $K_{\V\V}$ , $k$, $\V$, $\epsilon$}
 \KwResult{Sensor Selection $\A$}
 initialisation\;
 $\A = \emptyset$ \\
 \lForEach{$y \in \S $}{$\delta_y \leftarrow $ \ref{equ:initLocal} }
 begin\;
 \For{$j \leftarrow 1$ \KwTo $k$}{
  	\lForEach{$y \in \S \backslash \A $}{$current_y \leftarrow $ False }
 	\While{True}{
 	$y^* \leftarrow {\arg \max}_{y \in \S \backslash A } \delta_y$ \\
 	$\A \leftarrow \A \cup y^* $ \\ 
	\lForEach{$y \in N(y^*,\epsilon) $}{$\delta_y \leftarrow $ \ref{equ:updateLocal} }
 	
 	}
	
 }
\caption{Local Kernel Algorithm}
\label{alg:local}
\end{algorithm} 


\citet{krause_near-optimal_2008} proves that this algorithm approximates the optimal solution with complexity $\mathcal{O}(nd^3 + nk + kd^4)$. The solution found by this algorithm is close to the real optimum within given bounds. \\

This two strategies can be combined in order to make the problem even more scalable. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Progress}

This chapter exposes the work done until now in the project. It will focus on the data exploration done, the issues encountered in the covariance estimation, and the results of the algorithms developed in the sensor optimisation procedure.  All the commented codes are available on the Github repository of the project. 


\section{Framework}

In order to implement the algorithm and manipulate the VTK files of the dataset and I relied on Python 3.7 scripts that I runned the cluster of the DSI.  \\
 
 I have implemented a set of function to load and save the dataset and various intermediary computation results. Additional methods for the quick manipulation and the visualisation of the space. And functions that implement the covariance matrix estimate and the optimisation algorithms. 

\section{Data Exploration}

As a main dataset we use a the simulation results on an unstructured mesh of measurements with a very high density in the low middle of the space. The simulation used has $100'040$ points over 988 timestamps. Several fields are present in the simulation : the \textbf{pressure}, the \textbf{tracer} concentration, the \textbf{background tracer} concentration and the \textbf{velocity}. We represent in the next plots 2 cuts made to visualise the tracer and the tracer background. \\ 
\begin{figure}[hbt]
  \includegraphics[width=\linewidth]{figures/Tracer/tracer_cut_z_639}
  \caption{Cut of tracer at t = 639 }
\end{figure}

\begin{figure}[hbt]
  \includegraphics[width=\linewidth]{figures/Tracer/tracerBackground_cut_z_106}
  \caption{Cut of tracer background at t = 106 }
\end{figure}


\section{Covariance Estimation}

In order to test optimisation algorithm I have make the choice to first use a subset of the whole dataset containing $2'488$ point and created by cropping the space to a cube of $30\times30\times30\times$ at the center of the original space. On  this subspace I have made the choice to compute an isotropic covariance matrix (Exponential kernel,  Matern 3/2 and 5/2) with a length-scale of $l=3$ for testing the performance of the optimisation algorithm. \\


I had initially tried it with the sample covariance. As a matter of fact, it did'nt work properly, resulting in problems due to the positive-definiteness and the singularity of the sample covariance. \\ 

Also, the memory taken by those covariance matrix shows the necessity of the usage of a sparse matrix representation for the covariance.  \\ 


\section{Sensor Optimisation}

We have tested two algorithm exposed earlier : \\
  
The standard greedy approach (algorithm \ref{alg:greedy} ). \\
The Lazy approach developed using python heaps. (algorithm \ref{alg:lazy}). This version is as expected much faster than the standard version.  \\ 

We plot in the following figure the results obtained in the setting described earlier for a number of sensors $k=10$. 

\begin{figure}[hbt]
  \includegraphics[width=\linewidth]{figures/SensorOpt/sensor10_matern52_15x15x30cube_1}
  \caption{Position of the optimal set of 10 sensors}
\end{figure}



\chapter{Conclusion}

In this report we have stated some of the research that was done in order to solve the sensor position optimisation problem. \\ 

The biggest challenge here is to make the algorithm scalable for optimising over $100'000$ sensor locations. It requires to develop p strategies to make the GPs scalable. The main other challenge is to find a method to accurately estimate the covariance matrix. The theory enabling this scalability still need to be developed \\

I have implemented some of the optimisation algorithm proposed on a smaller dataset and with a poor estimation of the covariance function. It has given consistent results and has proven that once the main challenges, described earlier are solved, we will have a good optimisation algorithm for that kind of setups. 

 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\chapter{Further Developements}


%% bibliography
\bibliographystyle{apa}
\bibliography{bibliography}

\end{document}
