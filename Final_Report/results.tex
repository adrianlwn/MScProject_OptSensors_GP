\chapter{Results and Analysis} \label{chap:results}
After implementing the main analysis and optimisation parts we have run several experiments to find the most suitable points. For that we have explored different approaches in the estimation of the covariance function, we have compared the different algorithm on a subset of the data, we have run the optimisation on the full dataset and finally, we have validated those results with a Data Assimilation Procedure. 


%%%%%%%%% COVARIANCE %%%%%%%%
\section{Covariance}

As we have seen, our approach relies heavily on a properly defined covariance estimation. We have applied several approaches in order to find a good covariance and we are presenting them in this section. \\

For this section, we present results obtained by taking the pre-selected dataset of $23'643$ locations. 

\subsection{Sample Covariance}

The simplest covariance estimation is the Sample (Empirical) Covariance. This Matrix is a very poor estimate, as it is singular and not positive definite and poorly conditioned. It makes it impossible to invert to use it for the GPs of our optimisation problem. \\ 

We compute the determinant of the matrix using \texttt{numpy.linalg.slogdet} for the stability of this function. It indicates us that the determinant is clearly negative, and the the logarithm of the determinant's absolute value is : $-1'399'936.286$. The largest eigenvalue is : $\lambda_1 = 0.0219$ and the smallest eigenvalue is : $\lambda_p = -6.8929 \cdot 10^{-18}$.

We plot in figure \ref{fig:cov:emp:eigs} the eigen-decomposition of this sample covariance matrix.

\begin{figure}[h!]
\centering
    \includegraphics[width=0.7\linewidth]{figures/Covariance/Tracer_23643/cov_emp_eigenval_loglog}
    \caption{Empirical Covariance : Eigenvalues}
    \label{fig:cov:emp:eigs}
\end{figure}

So the sample covariance matrix can't be directly used for our optimisation.  

\subsection{Shrinkage Covariance}

In this section we show the results of the class of shrinkage covariance estimators. We use a simple \textbf{shrinkage} with shrinkage constant $\rho = 0.1$,  $\rho = 0.5$ and $\rho = 0.8$,  the \textbf{Ledoit-Wolf} Shrinkage and the \textbf{OAS} Shrinkage. 

\begin{table}[h]
\centering
    \begin{tabular}{l|ccccc}
     \toprule
        & SignDet & LogDet & $\lambda_1$ & $\lambda_p$ & $\rho$ \\ \midrule
        Empirical & ($-$) & $-1'399'936.28$ &  $0.0219$ & $-6.8929 \cdot 10^{-18}$ & -\\
        Shrinkage $\rho=0.1$ & ($+$) & $-351'807.64$ &  $0.0197$ &$3.3603\cdot 10^{-7}$ & $0.1$\\
        Shrinkage $\rho=0.5$ & ($+$) & $-314'035.58$ &  $0.0109$  &$1.6801 \cdot 10^{-6}$ & $0.5$\\
        Shrinkage $\rho=0.8$ & ($+$) & $-303'046.20$ &  $0.00438$  &$2.6882 \cdot 10^{-6}$ & $0.8$\\
        Ledoit-Wolf & ($+$) & $-406'320.19$&  $0.0217$ & $3.2867\cdot 10^{-8}$ & $0.0097811$\\
        OAS & ($+$) & $-408'903.33$ & $0.0217$ & $2.9435\cdot 10^{-8}$ & $0.0087595$ \\  \bottomrule
    \end{tabular}
    \caption{Shrinkage Comparison}
\end{table}

\begin{figure}[h!]
\centering
    \includegraphics[width=0.8\linewidth]{figures/Covariance/Tracer_23643/cov_all_eigenval_loglog_zoom}
    \caption{Eigenvalues Covariance Comparison}
    \label{fig:cov:comparison:eigs}
\end{figure}

As we can see the shrinkage algorithm allows to have a positive definite definite matrix that is very close the the original covariance matrix. It shares a lot a eigenvalues with the sample covariance matrix. This varies in function of the shrinkage constant $\rho$. For arbitrary values, $\rho = 0.1$,  $\rho= 0.5$ and  $\rho= 0.8$ we see that the smaller the shrinkage constant is, the closer we get to the sample covariance. For large constant value, we see that the upper part of the spectrum deteriorates completely, meaning that we don't have a good estimation anymore.  \\

For LW and OAS, the shrinkage variable computed is actually close to $0.1$. The spectrum are very close and the resulting in very close covariances matrixes. We see that the upper part is almost identical to the sample covariance matrix, but the lower part is truncated. As OAS gives us theoretically an optimal $\rho$ with very small error, we decide to use the \textbf{OAS estimation} of the covariance matrix for the optimisation procedure to follow.

%\subsection{Covariance on other fields}

\subsection{Arbitrary Covariance Function}

\todo{Maybe Remove subsection}





%%%%%%%%% COMPARISON %%%%%%%%

\section{Comparison Optimisation Algorithms}

We have defined the three algorithms allowing for a near-optimal placement of the sensors. We have also defined an approximation of the GPs. In order to compare their performance and measure their limitation, we have run them on a small dataset, a subset of our main one. 
\subsection{Conditions of the Experiment}

We have defined a sphere of radius $25$m, centred close to the centre, in the middle of the propagation beam, at position $[60,35,0]$ in which are contained $3'130$ points of the mesh. By taking the intersection between those points and our selection defined in \ref{sec:preselection} , we can reduce the number of points $|S|$ to $1'295$.  The candidates are presented in figure      \ref{fig:smallset:position}.
 \\

\begin{figure}[h!]
\centering
    \includegraphics[width=0.8\linewidth]{figures/CompAlg/3rd/non_centered_60.35.0/candidates_screenshot}
    \caption{Small Subset Candidates}
    \label{fig:smallset:position}
\end{figure}

We compute the OAS covariance for this dataset and we will use in in the rest of the experience. The eigenvalues decomposition is shown for both the empirical covariance and the OAS estimate in figure \ref{fig:small_cov_eig}. As shown previously we have a spectrum that is close to the original sample covariance, but with a truncated lower part. \\

We had a sample covariance that was \textbf{negative definite} with \textit{logDet} of $-39'755.06$ This allows to get a \textbf{positive definite} covariance matrix with \textit{logDet} of $-19'447.95$.  \\

\begin{figure}[h!]
\centering
    \includegraphics[width=0.8\linewidth]{figures/CompAlg/covarianceEmpOAS}
    \caption{Spectrum of Empirical and OAS covariances for small dataset}
    \label{fig:small_cov_eig}
\end{figure}

Next, we are going to optimise in this space $k=10$ points with the different algorithms and compare them based on their computation time and their distance between the datasets using the NN distance defined in \ref{equ:distNN} . 

\subsection{Greedy and Lazy Optimisation}

First, we optimise using the two first algorithms, the \textbf{greedy} and the \textbf{lazy} versions of the near-optimal sensor positioning algorithms. We will consider the output of the first algorithm as the reference for this comparison. This is because the first algorithm is the most reliable version as it covers every candidate point in its loops and relies on full GP implementation.  \\


We observe immediately that output sets given by the two methods are the same. The main difference is as expected the computation time which is much larger in the first algorithm. The Greedy Optimisation gives a result after $1043.27$s and the \textit{lazy} Optimisation gives results after $153.51 $s. The second algorithm is, for this small dataset, seven times faster than the greedy one: this is a significative difference. \\ 

The location of those points optimal points $\A^*$ is shown in green in the figure \ref{fig:opt_small} and table \ref{tab:opt_small}. We observe that the points are well spread across space and along the wind direction. As the wind is propagating the tracer concentration, it makes sense that the sensors must be placed along this direction. \\

\begin{figure}[h!]
\centering
    \includegraphics[width=0.8\linewidth]{figures/CompAlg/3rd/non_centered_60.35.0/optimal_screenshot}
    \caption{Optimal Points Location : Illustation}
    \label{fig:opt_small}
\end{figure}

\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{l|rrrrrrrrrr}
\toprule
$\A^*$ &  52731 &  47876 &  3078  &  19782 &  26045 &  30511 &  26754 &  81507 &  11608 &  3903  \\
\midrule
X &  61.31 &  43.55 &  77.31 &  38.75 &  48.23 &  50.48 &  52.36 &  43.10 &  82.40 &  62.01 \\
Y &  40.06 &  27.62 &  35.55 &  35.26 &  29.38 &  30.04 &  44.37 &  27.52 &  44.48 &  32.39 \\
Z &   1.52 &  16.40 &   1.45 &   1.80 &  19.53 &  11.79 &   1.62 &  10.23 &   1.96 &   0.20 \\
\bottomrule
\end{tabular}
\caption{Optimal Points Locations : Data}
\label{tab:opt_small}
\end{table}

\subsection{Local Kernels Optimisation}

The 3rd Algorithm that we are going to use is the one that is scalable to larger datasets. This scalability is dependent on the parameter $\epsilon$, the thresholding parameter of the covariance. As a consequence the number of selected covariates  $|N(y_{opt},\epsilon)| \leq d $ is fluctuating and influences greatly the speed of the algorithm. It is directly linked to the GP and the size of the covariance matrix to invert.  \\ 

This is why the threshold $\epsilon$ has to be chosen carefully. We are going to show that the speed, the NN distance and the parameter d varies according to $\epsilon$, before choosing a value that will be used for the full-scale optimisation. We compute of a range of values for $\epsilon$ all those results and show them in table \ref{tab:comp:results} and in figure \ref{fig:comp:results}. \\



\begin{table}[h!]
\centering
\scriptsize
\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c}
  \toprule
  Threshold $\epsilon$ & $10^{-10} $ &  $10^{-9}$ & $10^{-8}$ & $10^{-7}$ & $10^{-6}$ & $10^{-5}$ & $10^{-4}$ & $10^{-3}$ & $10^{-2}$ & $10^{-1}$ \\
    \midrule
  Distance [m]       &    0.00 &    0.00 &    0.00 &    0.00 &   1.248 &  2.783 & 4.693 & 11.058 & 11.058 & 11.058 \\
Time [s]       & 1660.85 & 1617.92 & 1500.63 & 1151.68 &  520.36 &  60.88 &  2.44 &   1.74 &   3.25 &   2.13 \\
Average d & 1289.40 & 1288.20 & 1284.80 & 1245.60 & 1029.60 & 510.50 & 15.20 &   0.00 &   0.00 &   0.00 \\
  \bottomrule
\end{tabular}
\caption{Local Kernel Results}
\label{tab:comp:results}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[height=0.33\linewidth]{figures/CompAlg/3rd/non_centered_60.35.0/comp_dist}
~
\includegraphics[height=0.33\linewidth]{figures/CompAlg/3rd/non_centered_60.35.0/comp_Time}
~
\includegraphics[height=0.33\linewidth]{figures/CompAlg/3rd/non_centered_60.35.0/comp_d_opt}
\caption{Local Kernels : Distance, Time and d, in function of $\epsilon$}
\label{fig:comp:results}
\end{figure}

We see that computation time is correlated with the average number of correlates d. For a threshold bellow $10^{-7}$, we see that the algorithm selects almost every point available and therefore we have computation times of the order of the greedy algorithm. The optimised set of points is then also the same as the greedy and lazy algorithms.\\

By increasing the threshold, we find that the computation time and the number of covariates decrease at the expense of the accuracy represented by the increase in the NN distance between this dataset and the optimal one. We observe also that for $\epsilon > 10^{-3}$, the number of covariates selected d is equal to zero which means that the algorithm updates every position at each iteration and uses for the optimisation only the values of the original covariance, without conditioning them, as the observed set is empty.  \\

Therefore we need to \textbf{fix the threshold} at around $ \epsilon_{opt} = 10^{-6}$, so that the error is still small and the computation time substantially reduced. \\ 


By moving the centre of the selection sphere, we observe that the optimal threshold is very different from one zone to the other. For example, if we place the subset in a zone where the tracer data is almost always null, we see that the threshold needs to be much smaller to get results. This is why the value we have chosen was optimised for an area where there is some data. 


\subsection{Approximated Gaussian Processes}

Alternatively, we use the approximation method developed earlier relying on the TSVD of the data, applied to algorithm 2. We proceed to the optimisation of the dataset using different values for the truncation parameter $\tau$. We then plot the computation time and the set distance to the results of the greedy algorithm in function of the truncation parameter in figure \ref{fig:small_set:tsvd}.

\begin{figure}[h]
\centering
\includegraphics[height=0.33\linewidth]{figures/CompAlg/tsvd/dist_trunc}
~
\includegraphics[height=0.33\linewidth]{figures/CompAlg/tsvd/time_trunc}
\caption{TSVD Approximation : Distance and Time in function of $\tau$}
\label{fig:small_set:tsvd}
\end{figure}

As we can see the computation time increases exponentially with $\tau$ and it is then reduced when $\tau$ gets larger than $n=988$.  For the distance between the sets and the optimal results of algorithm 1, we see that it is quite low at the beginning, is reaches a minimum at $\tau_{opt} = 25$ and it then increases this the truncation parameter. We will keep this method for trying it on the full dataset in the next section. 

%
%
%\begin{table}
%\centering
%\scriptsize 
%    \begin{tabular}{lrrrrrrrrrrrrrrrrrrrrrrrrrrr}
%\hline
% &   2   &   5   &   8   &   10  &   11  &   12  &   14  &   20  &   23  &   25  &   28  &   30  &   40  &   50  &   60  &   70  &   80  &    90  &    100 &    150 &    200 &    300 &    400 &    500 &    600 &     800 &   989 \\
%\hline
%dist & 59.87 & 55.30 & 53.91 & 53.66 & 53.15 & 44.17 & 53.63 & 45.27 & 43.13 & 42.82 & 44.74 & 52.78 & 62.10 & 58.05 & 64.22 & 71.09 & 59.93 &  65.64 &  69.20 &  51.60 &  57.82 &  61.12 &  63.61 &  68.67 &  77.67 &   79.92 & 96.42 \\
%time & 43.52 & 46.03 & 45.33 & 44.59 & 48.38 & 44.93 & 47.97 & 51.90 & 50.48 & 56.97 & 56.55 & 61.21 & 69.06 & 77.49 & 79.66 & 80.56 & 99.81 & 105.75 & 114.13 & 161.96 & 211.30 & 305.01 & 448.92 & 567.17 & 741.89 & 1018.80 & 49.40 \\
%\hline
%\end{tabular}
%\end{table}

%
%\subsection{Arbitrary Covariance}
%
%Alternatively, we define the covariance using an \textit{isotropic} and \textit{stationary} kernel function with arbitrary parameters. We chose for this experiment the 5/2 Mat√©rn kernel. Unrelated to the data !!
%
%\todo{Add in literature review // Implementation the Kernels definitions}
%






%\todo{Add section on the abritrary kernels results}




%%%%%%%%% OPTIMISATION %%%%%%%%

\section{Optimisation Results}

After the analysis of different algorithm results on a small sample dataset, we are now going to use the most scalable algorithms with the appropriate parameters on the full dataset. The set of points used is the preselected dataset containing $23'643$ locations. \\

In this section, we will present the results of two optimisations, compare them in term of computation speed and proximity. 

\subsection{Local Kernel Algorithm} \label{sec:res:localK}

We first use the Local Kernel Algorithm \ref{alg:local}. The covariance between the points is computed using the OAS estimator. \\


For this experiment we used the previously defined value of threshold : $\epsilon = 10^{-6}$. Such that the number of points highly correlated to the last positioned sensor is $|N(y_{opt},\epsilon)| \leq d $. \\ 

The algorithm computes the following results in $23797.2$ seconds or $6.61$ hours. \\

We are computing for each iteration the size of this set. This number is directly linked to the size of the covariance matrix being inverted in the GPs. As we are placing 10 sensors, we have 10 values of this quantity that is computed at each iteration of the main loop. Results can be found in table \ref{tab:full:d_opt}. The average value is $4'374.7$. This shows that indeed the optimisation problem is solved in a reduced time as it doesn't use all the $23'643$ locations of our dataset, but $18.50$\% of it.   \\

\begin{table}[h]
    \centering
    \begin{tabular}{l|cccccccccc}
    \toprule
    Sensor & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\     \midrule
        $|N(y_{opt},\epsilon)|$ & 4338 & 4938 & 5066 & 5362 & 4239 & 4164 & 3377 & 4632 & 3511 & 4120 \\     \bottomrule

    \end{tabular}
    \caption{Number of highly correlated points at each iteration}
    \label{tab:full:d_opt}
\end{table}


We visualise the position of the optimal set of sensor points $\A^*$ at different scales in the figures \ref{fig:full_set:position:all}, \ref{fig:full_set:position:top} and \ref{fig:full_set:position:zoom}. The coordinates of the points are expressed in the table \ref{tab:full:data}. \\


\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{l|rrrrrrrrrr}
\toprule
$\A^*$ &  56588 &  52731 &  73959 &  43278 &  3078  &  19782 &  56257 &  55640 &  10357 &  54786 \\ \midrule
X &  41.61 &  61.31 &  30.32 &  22.79 &  77.31 &  38.75 &  91.19 &  50.10 &  62.13 &   1.62 \\
Y &  27.21 &  40.06 &  26.06 &  25.81 &  35.55 &  35.26 &  35.16 &  29.00 &  45.23 &  19.99 \\
Z &  16.73 &   1.52 &  11.19 &  11.79 &   1.45 &   1.80 &   1.82 &  16.41 &   1.64 &  13.81 \\
\bottomrule
\end{tabular}
\caption{Optimal Points Locations : Data}
\label{tab:full:data}
\end{table}



\begin{figure}[h!]
\centering
\includegraphics[width=0.7\linewidth]{figures/MainOptimResults/alg3opteps10-6_sideall_screenshot}
\caption{Position of the Optimal Set : Main View}
\label{fig:full_set:position:all}
\end{figure}



\begin{figure}[h!]
\centering
\includegraphics[width=0.7\linewidth]{figures/MainOptimResults/alg3opteps10-6_top_screenshot}
\caption{Position of the Optimal Set : Top View}
\label{fig:full_set:position:top}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.7\linewidth]{figures/MainOptimResults/alg3opteps10-6_zoom_screenshot}
\caption{Position of the Optimal Set : Close View}
\label{fig:full_set:position:zoom}
\end{figure}


\paragraph{Description}

The location of our optimal set of points is split between points situated on the buildings (5 points) and around the ground level (5 points). They are situated in the middle of the tracer concentration beam and are close to the centre of the domain where the mesh density is maximal. \\ 

The points are roughly aligned in the wind direction, which seems to be a good thing because we have a propagation originating at the centre and with wind in the east direction. 


\subsection{Lazy Algorithm with TSVD} \label{sec:res:TSVD}

We apply on the same dataset the 2nd Algorithm with the approximated GP that we defined earlier. We use the truncation parameter $\tau_{opt} = 25$. The results we obtain are displayed in table \ref{tab:tsvd:data} and figures \ref{fig:full_set_tsvd:position:top} and \ref{fig:full_set_tsvd:position:zoom} along with the previous optimal results. \\ 
 
This algorithm computes the following results in $18083.87$ seconds or $5.02$ hours. 
 
 \paragraph{Description}

Here again the location of our optimal set of points is split between points situated on the buildings (3 points) and around the ground level (7 points). The locations are more spread than the other set of points. Two locations are at the border the the pre-selected dataset. The distance between this set and the previous one is : $13.84$m which is quite important.  \\ 



\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{l|rrrrrrrrrr}
\toprule
$\A^*$ &  38726 &  14276 &  91348 &  5338  &  40994 &  29626 &  65851 &  65734 &  851   &  2293  \\
\midrule
X & -50.50 &  35.63 &  43.05 & 137.58 &  80.34 &  27.79 &  49.54 &  54.59 &  77.54 &  62.03 \\
Y &  48.85 &  58.69 &  27.51 &  55.78 &  76.20 &  26.24 &  28.88 &  25.42 &  34.82 &  41.92 \\
Z &  14.84 &   0.20 &   7.76 &   0.20 &   0.20 &  12.06 &  17.65 &   3.80 &   0.20 &   0.20 \\
\bottomrule
\end{tabular}
\caption{Optimal Points Locations TSVD : Data}
\label{tab:tsvd:data}
\end{table}


\begin{figure}[h!]
\centering
\includegraphics[width=0.7\linewidth]{figures/MainOptimTSVD/tsvd+3rd_position_top}
\caption{Position of the Optimal Set : Main View - $\tau = 25$}
\label{fig:full_set_tsvd:position:top}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.7\linewidth]{figures/MainOptimTSVD/tsvd+3rd_position_side}
\caption{Position of the Optimal Set : Main View $\tau = 25$}
\label{fig:full_set_tsvd:position:zoom}
\end{figure}







%%%%%%%%% OPTIMISATION %%%%%%%%

\section{Application to Variational DA}


As we have seen, one way of applying the results to a real situation is to proceed to a Variational Data Assimilation of the points selected. By taking results of a simulation and applying DA on our selected points we can try to see if the results are usable in the context of the MAGIC project. 

\subsection{Parameters of the Experiment} 

For this experiment we chose to consider the same simulation data than previously: the 3D smallLSBU.  We choose as the observation state, the step $988$ of the simulation, and as background state, the step $100$ of the simulation. \\

We compute the Variational DA on the 10 optimal points selected by the algorithms and compute with those a background MSE and a DA MSE. \todo{mention those in the implementation section}

\subsection{VarDA on Local Kernel Results}

First, we run the VarDa Algorithm and obtain the following. 
\begin{table}[h]
\centering
	\begin{tabular}{c|c|c}
	\toprule
		  MSE [xDA] & MSE [xB] &  Imp. [xB - xDA] \\ \midrule
		 $7.7375 \cdot 10^{-17}$ &  $0.9923$ & $0.9923$ \\ \bottomrule
	\end{tabular}
	\caption{VarDA on Local Kernel Results. }
\end{table}

\paragraph{Comparison to Random}

In order to compare those results to an other set of points we proceed as follows. Around each single point of $\A^*$, we select randomly (uniformly) a point located within a radius of $R=5m$. This allows us to compare the results to some random set that is close to the optimum. We proceed to $1'000$ different random selection and compute the average errors and their standard deviations, stated in table \ref{tab:varDa:random}. \\


\begin{table}[h]
\centering
\begin{tabular}{l|cc}
\toprule
               & Average & Std Dev \\ \midrule
MSE [xDA]      & $6.1008 \cdot 10^{-17}$  & $4.1680 \cdot 10^{-17}$  \\
MSE [xB]       & $0.6538$  & $0.1718$  \\
Imp. [xB - xDA]  &  $0.6538$  & $0.1718$ \\ \bottomrule
\end{tabular}
	\caption{VarDA on $1'000$ Near Random Points : Local Kernel Results}
	\label{tab:varDa:random}
\end{table}

\paragraph{Analysis}

As we see it seems that the random points, even if they are close the optimal seem to be giving better results than our optimal set of points $\A^*$. 
\todo{Finish Analysis}



\subsection{VarDA on Lazy Algorithm with TSVD Results}

We then run the same algorithm on the optimal set found with the lazy algorithm combined with TSVD GP approximation. 

\begin{table}[h]
\centering
	\begin{tabular}{c|c|c}
	\toprule
		  MSE DA & MSE Background & Improvement \\ \midrule
		$9.8945 \cdot 10^{-17}$  &  $1.1354$ & $1.1354$ \\ \bottomrule
	\end{tabular}
	\caption{VarDA on TSVD Results. }
\end{table}

\paragraph{Comparison to Random}

In order to compare those results to an other set of points we proceed as previously. The results for $1'000$ random sets sampled at a $10$m radius from each optimal points are given in table 	\ref{tab:TSVD:random}. 
 \\


\begin{table}[h]
\centering
\begin{tabular}{l|cc}
\toprule
               & Average & Std Dev \\ \midrule
MSE VarDA      & $6.6843 \cdot 10^{-17}$  & $4.9458 \cdot 10^{-17}$  \\
MSE Background & $0.6725$  & $0.2148$  \\
Improvement    & $0.6725$  & $0.2148$ \\ \bottomrule
\end{tabular}
	\caption{VarDA on 500 Random Points : TSVD Results}
	\label{tab:TSVD:random}
\end{table}
\todo{Maybe Update Results}
\paragraph{Analysis}

As we see it seems that the random points, even if they are close the optimal seem to be giving better results than our optimal set of points $\A^*$. 
\todo{Finish Analysis}



\subsection{Limitations of the Application}

As we have seen the results given by this simple VarDA application are not very satisfying. We observe clearly that the randomly selected points give in average better results for DA than our optimal points. \\

We have to keep in mind the the DA Algorithm used is computing the MSE only on the 10 points of interest. We do not study here the propagation after the DA, so we are not really able to see the impact of our selected set on the rest of the locations of the space. \\
Furthermore, we have consider the DA as an application of our near-optimal sensor positioning algorithm. The criterion we have optimised to find the optimal points has no proven link with the DA. Not taking into account the dynamics of the space. Assumption of samples are independant and identically distributed and samples of the sample gaussian probability distribution. 

\todo{Finish limitation subsection} 