
@article{arcucci_optimal_2019,
	title = {Optimal reduced space for {Variational} {Data} {Assimilation}},
	volume = {379},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999118307095},
	doi = {10.1016/j.jcp.2018.10.042},
	abstract = {Data Assimilation (DA) is an uncertainty quantification technique used to incorporate observed data into a prediction model in order to improve numerical forecasted results. Variational DA (VarDA) is based on the minimisation of a function which estimates the discrepancy between numerical results and observations. Operational forecasting requires real-time data assimilation. This mandates the choice of opportune methods to improve the efficiency of VarDA codes without loosing accuracy. Due to the scale of the forecasting area and the number of state variables used to describe the physical model, DA is a big data problem. In this paper, the Truncated Singular Value Decomposition (TSVD) is used to reduce the space dimension, alleviate the computational cost and reduce the errors. Nevertheless, a consequence is that important information is lost if the truncation parameter is not properly chosen. We provide an algorithm to compute the optimal truncation parameter and we prove that the optimal estimation reduces the illconditioning and removes the statistically less significant modes which could add noise to the estimate obtained from DA. In this paper, numerical issues faced in developing VarDA algorithm include the ill-conditioning of the background covariance matrix, the choice of a preconditioning and the choice of the regularisation parameter. We also show how the choice of the regularisation parameter impacts on the efficiency of the VarDA minimisation computed by the L-BFGS (Limited {\textendash} Broyden Fletcher Goldfarb Shanno). Experimental results are provided for pollutant dispersion within an urban environment.},
	language = {en},
	urldate = {2019-04-15},
	journal = {Journal of Computational Physics},
	author = {Arcucci, Rossella and Mottet, Laetitia and Pain, Christopher and Guo, Yi-Ke},
	month = feb,
	year = {2019},
	pages = {51--69},
	file = {Arcucci et al. - 2019 - Optimal reduced space for Variational Data Assimil.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/initial papers/Arcucci et al. - 2019 - Optimal reduced space for Variational Data Assimil.pdf:application/pdf}
}

@article{krause_near-optimal_2008,
	title = {Near-{Optimal} {Sensor} {Placements} in {Gaussian} {Processes}: {Theory}, {Efficient} {Algorithms} and {Empirical} {Studies}},
	abstract = {When monitoring spatial phenomena, which can often be modeled as Gaussian processes (GPs), choosing sensor locations is a fundamental task. There are several common strategies to address this task, for example, geometry or disk models, placing sensors at the points of highest entropy (variance) in the GP model, and A-, D-, or E-optimal design. In this paper, we tackle the combinatorial optimization problem of maximizing the mutual information between the chosen locations and the locations which are not selected. We prove that the problem of finding the configuration that maximizes mutual information is NP-complete. To address this issue, we describe a polynomial-time approximation that is within (1 - 1/e) of the optimum by exploiting the submodularity of mutual information. We also show how submodularity can be used to obtain online bounds, and design branch and bound search procedures. We then extend our algorithm to exploit lazy evaluations and local structure in the GP, yielding significant speedups. We also extend our approach to find placements which are robust against node failures and uncertainties in the model. These extensions are again associated with rigorous theoretical approximation guarantees, exploiting the submodularity of the objective function. We demonstrate the advantages of our approach towards optimizing mutual information in a very extensive empirical study on two real-world data sets.},
	language = {en},
	author = {Krause, Andreas and Singh, Ajit and Guestrin, Carlos},
	year = {2008},
	pages = {50},
	file = {Krause et al. - 2008 - Near-Optimal Sensor Placements in Gaussian Process.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/initial papers/Krause et al. - 2008 - Near-Optimal Sensor Placements in Gaussian Process.pdf:application/pdf}
}

@article{song_natural_2018,
	title = {Natural ventilation in cities: the implications of fluid mechanics},
	volume = {46},
	issn = {0961-3218, 1466-4321},
	shorttitle = {Natural ventilation in cities},
	url = {https://www.tandfonline.com/doi/full/10.1080/09613218.2018.1468158},
	doi = {10.1080/09613218.2018.1468158},
	abstract = {Research under the Managing Air for Green Inner Cities (MAGIC) project uses measurements and modelling to investigate the connections between external and internal conditions: the impact of urban airflow on the natural ventilation of a building. The test site was chosen so that under different environmental conditions the levels of external pollutants entering the building, from either a polluted road or a relatively clean courtyard, would be significantly different. Measurements included temperature, relative humidity, local wind and solar radiation, together with levels of carbon monoxide (CO) and carbon dioxide (CO2) both inside and outside the building to assess the indoor{\textendash}outdoor exchange flows. Building ventilation took place through windows on two sides, allowing for single-sided and crosswind-driven ventilation, and also stackdriven ventilation in low wind conditions. The external flow around the test site was modelled in an urban boundary layer in a wind tunnel. The wind tunnel results were incorporated in a largeeddy-simulation model, Fluidity, and the results compared with monitoring data taken both within the building and from the surrounding area. In particular, the effects of street layout and associated street canyons, of roof geometry and the wakes of nearby tall buildings were examined.},
	language = {en},
	number = {8},
	urldate = {2019-04-15},
	journal = {Building Research \& Information},
	author = {Song, Jiyun and Fan, S. and Lin, W. and Mottet, L. and Woodward, H. and Davies Wykes, M. and Arcucci, R. and Xiao, D. and Debay, J.-E. and ApSimon, H. and Aristodemou, E. and Birch, D. and Carpentieri, M. and Fang, F. and Herzog, M. and Hunt, G. R. and Jones, R. L. and Pain, C. and Pavlidis, D. and Robins, A. G. and Short, C. A. and Linden, P. F.},
	month = nov,
	year = {2018},
	pages = {809--828},
	file = {Natural ventilation in cities the implications of fluid mechanics.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/initial papers/Natural ventilation in cities the implications of fluid mechanics.pdf:application/pdf}
}

@inproceedings{guestrin_near-optimal_2005,
	address = {Bonn, Germany},
	title = {Near-optimal sensor placements in {Gaussian} processes},
	isbn = {978-1-59593-180-1},
	url = {http://portal.acm.org/citation.cfm?doid=1102351.1102385},
	doi = {10.1145/1102351.1102385},
	abstract = {When monitoring spatial phenomena, which are often modeled as Gaussian Processes (GPs), choosing sensor locations is a fundamental task. A common strategy is to place sensors at the points of highest entropy (variance) in the GP model. We propose a mutual information criteria, and show that it produces better placements. Furthermore, we prove that finding the configuration that maximizes mutual information is NP-complete. To address this issue, we describe a polynomial-time approximation that is within (1 - 1/e) of the optimum by exploiting the submodularity of our criterion. This algorithm is extended to handle local structure in the GP, yielding significant speedups. We demonstrate the advantages of our approach on two real-world data sets.},
	language = {en},
	urldate = {2019-04-15},
	booktitle = {Proceedings of the 22nd international conference on {Machine} learning  - {ICML} '05},
	publisher = {ACM Press},
	author = {Guestrin, Carlos and Krause, Andreas and Singh, Ajit Paul},
	year = {2005},
	pages = {265--272},
	file = {Guestrin et al. - 2005 - Near-optimal sensor placements in Gaussian process.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/initial papers/Guestrin et al. - 2005 - Near-optimal sensor placements in Gaussian process.pdf:application/pdf}
}

@article{law_data_2015,
	title = {Data {Assimilation}: {A} {Mathematical} {Introduction}},
	shorttitle = {Data {Assimilation}},
	url = {http://arxiv.org/abs/1506.07825},
	abstract = {These notes provide a systematic mathematical treatment of the subject of data assimilation.},
	urldate = {2019-04-15},
	journal = {arXiv:1506.07825 [math, stat]},
	author = {Law, K. J. H. and Stuart, A. M. and Zygalakis, K. C.},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.07825},
	keywords = {Mathematics - Dynamical Systems, Mathematics - Optimization and Control, Statistics - Methodology},
	file = {arXiv.org Snapshot:/Users/adrian/Zotero/storage/LKCPR6YU/1506.html:text/html;Law et al. - 2015 - Data Assimilation A Mathematical Introduction.pdf:/Users/adrian/Zotero/storage/SPAUV7FU/Law et al. - 2015 - Data Assimilation A Mathematical Introduction.pdf:application/pdf}
}

@article{arcucci_variational_2017,
	title = {On the variational data assimilation problem solving and sensitivity analysis},
	volume = {335},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999117300505},
	doi = {10.1016/j.jcp.2017.01.034},
	abstract = {We consider the Variational Data Assimilation (VarDA) problem in an operational framework, namely, as it results when it is employed for the analysis of temperature and salinity variations of data collected in closed and semi closed seas. We present a computing approach to solve the main computational kernel at the heart of the VarDA problem, which outperforms the technique nowadays employed by the oceanographic operative software. The new approach is obtained by means of Tikhonov regularization. We provide the sensitivity analysis of this approach and we also study its performance in terms of the accuracy gain on the computed solution. We provide validations on two realistic oceanographic data sets.},
	language = {en},
	urldate = {2019-04-20},
	journal = {Journal of Computational Physics},
	author = {Arcucci, Rossella and D'Amore, Luisa and Pistoia, Jenny and Toumi, Ralf and Murli, Almerico},
	month = apr,
	year = {2017},
	pages = {311--326},
	file = {Arcucci et al. - 2017 - On the variational data assimilation problem solvi.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Data Assimilation/Arcucci et al. - 2017 - On the variational data assimilation problem solvi.pdf:application/pdf}
}

@article{kouichi_optimization_2016,
	series = {Air {Pollution} {XXIV}},
	title = {Optimization of sensor networks for the estimation of atmospheric pollutants sources},
	volume = {207},
	url = {https://hal.archives-ouvertes.fr/hal-01593828},
	doi = {10.2495/AIR160021},
	abstract = {This study describes a process to design a sensor network. This network could include: wireless mobile sensors deployed by first responders in hazardous material operations, stationary sensors used to protect an area against accidental, or intentional, contaminations or stationary air quality monitoring stations. The objective of the network is the estimation (localization {\textendash} quantification) of releases sources. The design of such a network has an important issue in determining the optimal placement of sensors. This paper presents the first application of the renormalized data assimilation method to address this issue. It is associated with a classical optimization algorithm (simulate annealing) to solve the combinatory optimization problem consisting of finding the optimal configuration of sensors among a set of potential positions. Three scenarios, corresponding with three different cost functions, are proposed. The first one consists of optimizing the design of a network deployed in emergency situations. Experimental data from a wind tunnel experiment are used. The objective is to characterize the source to minimize error in measurement forecasts. The second one is to optimize the design of the same network but in a situation where the source can be anywhere in the domain. To that end, an entropic criterion is used. The last one consists of optimizing the design of a stationary network. The objective is to characterize the source with varying meteorological conditions (experimental meteorological data are used).},
	urldate = {2019-04-23},
	journal = {WIT Transactions on Ecology and the Environment},
	author = {kouichi, hamza and Turbelin, G and Ngae, P and Feiz, A. A. and Barbosa, E and Chpoun, A.},
	year = {2016},
	keywords = {network optimization, renormalized data assimilation, source characterization},
	pages = {11--21},
	file = {HAL PDF Full Text:/Users/adrian/Zotero/storage/F4NSMCG7/kouichi et al. - 2016 - Optimization of sensor networks for the estimation.pdf:application/pdf}
}

@article{nott_estimation_2002,
	title = {Estimation of nonstationary spatial covariance structure},
	volume = {89},
	issn = {0006-3444, 1464-3510},
	url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/89.4.819},
	doi = {10.1093/biomet/89.4.819},
	language = {en},
	number = {4},
	urldate = {2019-04-26},
	journal = {Biometrika},
	author = {Nott, D. J.},
	month = dec,
	year = {2002},
	pages = {819--829},
	file = {Nott - 2002 - Estimation of nonstationary spatial covariance str.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project/MScProject_OptSensors_GP/Gaussian Processes/Nott - 2002 - Estimation of nonstationary spatial covariance str.pdf:application/pdf}
}

@article{caselton_optimal_1984,
	title = {Optimal monitoring network designs},
	volume = {2},
	issn = {01677152},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0167715284900208},
	doi = {10.1016/0167-7152(84)90020-8},
	abstract = {The selection of a monitoring network is formulated as a decision problem whose solutions would then be optimal. The theory is apphed where the underlying field has a multivariate normal probability structure.},
	language = {en},
	number = {4},
	urldate = {2019-04-26},
	journal = {Statistics \& Probability Letters},
	author = {Caselton, W.F. and Zidek, J.V.},
	month = aug,
	year = {1984},
	pages = {223--227},
	file = {Caselton and Zidek - 1984 - Optimal monitoring network designs.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project/MScProject_OptSensors_GP/Gaussian Processes/Caselton and Zidek - 1984 - Optimal monitoring network designs.pdf:application/pdf}
}

@misc{noauthor_mutual_2019,
	title = {Mutual information},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Mutual_information&oldid=892560218},
	abstract = {In probability theory and information theory, the mutual information (MI) of two random variables is a measure of the mutual dependence between the two variables. More specifically, it quantifies the "amount of information" (in units such as shannons, commonly called bits) obtained about one random variable through observing the other random variable.  The concept of mutual information is intricately linked to that of entropy of a random variable, a fundamental notion in information theory that quantifies the expected "amount of information" held in a random variable.
Not limited to real-valued random variables like the correlation coefficient, MI is more general and determines how similar the joint distribution of the pair 
  
    
      
        (
        X
        ,
        Y
        )
      
    
    \{{\textbackslash}displaystyle (X,Y)\}
   is to the product of the marginal distributions of 
  
    
      
        X
      
    
    \{{\textbackslash}displaystyle X\}
   and 
  
    
      
        Y
      
    
    \{{\textbackslash}displaystyle Y\}
  . MI is the expected value of the pointwise mutual information (PMI).},
	language = {en},
	urldate = {2019-04-26},
	journal = {Wikipedia},
	month = apr,
	year = {2019},
	note = {Page Version ID: 892560218},
	file = {Snapshot:/Users/adrian/Zotero/storage/22PMAS6S/index.html:text/html}
}

@article{nott_estimation_2002-1,
	title = {Estimation of nonstationary spatial covariance structure},
	volume = {89},
	issn = {0006-3444, 1464-3510},
	url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/89.4.819},
	doi = {10.1093/biomet/89.4.819},
	abstract = {We introduce a method for estimating nonstationary spatial covariance structure from space-time data and apply the method to an analysis of Sydney wind patterns. Our method constructs a process honouring a given spatial covariance matrix at observing stations and uses one or more stationary processes to describe conditional behaviour given observing site values. The stationary processes give a localised description of the spatial covariance structure. The method is computationally attractive, and can be extended to the assessment of covariance for multivariate processes. The technique is illustrated for data describing the east-west component of Sydney winds. For this example, our own methods are contrasted with a geometrically appealing though computationally intensive technique which describes spatial correlation via an isotropic process and a deformation of the geographical space.},
	language = {en},
	number = {4},
	urldate = {2019-05-01},
	journal = {Biometrika},
	author = {Nott, D. J. and Dunsmuir, W. T. M.},
	month = dec,
	year = {2002},
	pages = {819--829},
	file = {Nott and Dunsmuir - 2002 - Estimation of nonstationary spatial covariance str.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Nott and Dunsmuir - 2002 - Estimation of nonstationary spatial covariance str.pdf:application/pdf}
}

@incollection{bordeaux_submodular_2013,
	address = {Cambridge},
	title = {Submodular {Function} {Maximization}},
	isbn = {978-1-139-17780-1},
	url = {https://www.cambridge.org/core/product/identifier/CBO9781139177801A031/type/book_part},
	language = {en},
	urldate = {2019-05-02},
	booktitle = {Tractability},
	publisher = {Cambridge University Press},
	author = {Krause, Andreas and Golovin, Daniel},
	editor = {Bordeaux, Lucas and Hamadi, Youssef and Kohli, Pushmeet and Mateescu, Robert},
	year = {2013},
	doi = {10.1017/CBO9781139177801.004},
	pages = {71--104},
	file = {Krause and Golovin - 2013 - Submodular Function Maximization.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Submodularity/Krause and Golovin - 2013 - Submodular Function Maximization.pdf:application/pdf}
}

@article{nemhauser_analysis_1978,
	title = {An analysis of approximations for maximizing submodular set functions{\textemdash}{I}},
	volume = {14},
	issn = {0025-5610, 1436-4646},
	url = {http://link.springer.com/10.1007/BF01588971},
	doi = {10.1007/BF01588971},
	abstract = {Let N be a finite set and z be a real-valued function defined on the set of subsets of N that satisfies z ( S ) + z ( T ) {\textgreater} - z ( S U T ) + z ( S n T ) for all S, T in N. Such a function is called submodular. We consider the problem maXscN \{z(S): IS[ {\textless}-K, z(S) submodular\}. Several hard combinatorial optimization problems can be posed in this framework. For example, the problem of finding a maximum weight independent set in a matroid, when the elements of the matroid are colored and the elements of the independent set can have no more than K colors, is in this class. The uncapacitated location problem is a special case of this matroid optimization problem. We analyze greedy and local improvement heuristics and a linear programming relaxation for this problem. Our results are worst case bounds on the quality of the approximations. For example, when z(S) is nondecreasing and z(0) = 0, we.show that a "greedy" heuristic always produces a solution whose value is at least 1 - [ ( K - 1 ) / K ] K times the optimal value. This bound can be achieved for each K and has a limiting value of ( e - l)/e, where e is the base of the natural logarithm.},
	language = {en},
	number = {1},
	urldate = {2019-05-02},
	journal = {Mathematical Programming},
	author = {Nemhauser, G. L. and Wolsey, L. A. and Fisher, M. L.},
	month = dec,
	year = {1978},
	pages = {265--294},
	file = {Nemhauser et al. - 1978 - An analysis of approximations for maximizing submo.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Submodularity/Nemhauser et al. - 1978 - An analysis of approximations for maximizing submo.pdf:application/pdf}
}

@inproceedings{leskovec_cost-effective_2007,
	address = {San Jose, California, USA},
	title = {Cost-effective outbreak detection in networks},
	isbn = {978-1-59593-609-7},
	url = {http://portal.acm.org/citation.cfm?doid=1281192.1281239},
	doi = {10.1145/1281192.1281239},
	abstract = {Given a water distribution network, where should we place sensors to quickly detect contaminants? Or, which blogs should we read to avoid missing important stories? These seemingly different problems share common structure: Outbreak detection can be modeled as selecting nodes (sensor locations, blogs) in a network, in order to detect the spreading of a virus or information as quickly as possible.},
	language = {en},
	urldate = {2019-05-02},
	booktitle = {Proceedings of the 13th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining  - {KDD} '07},
	publisher = {ACM Press},
	author = {Leskovec, Jure and Krause, Andreas and Guestrin, Carlos and Faloutsos, Christos and VanBriesen, Jeanne and Glance, Natalie},
	year = {2007},
	pages = {420},
	file = {Leskovec et al. - 2007 - Cost-effective outbreak detection in networks.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Submodularity/Other Applications/Leskovec et al. - 2007 - Cost-effective outbreak detection in networks.pdf:application/pdf}
}

@article{deisenroth_distributed_nodate,
	title = {Distributed {Gaussian} {Processes}},
	abstract = {To scale Gaussian processes (GPs) to large data sets we introduce the robust Bayesian Committee Machine (rBCM), a practical and scalable product-of-experts model for large-scale distributed GP regression. Unlike state-of-theart sparse GP approximations, the rBCM is conceptually simple and does not rely on inducing or variational parameters. The key idea is to recursively distribute computations to independent computational units and, subsequently, recombine them to form an overall result. Efficient closed-form inference allows for straightforward parallelisation and distributed computations with a small memory footprint. The rBCM is independent of the computational graph and can be used on heterogeneous computing infrastructures, ranging from laptops to clusters. With sufficient computing resources our distributed GP model can handle arbitrarily large data sets.},
	language = {en},
	author = {Deisenroth, Marc Peter and Ng, Jun Wei},
	pages = {10},
	file = {Deisenroth and Ng - Distributed Gaussian Processes.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Scalability/Deisenroth and Ng - Distributed Gaussian Processes.pdf:application/pdf}
}

@book{cressie_statistics_1993,
	address = {New York},
	series = {Wiley series in probability and mathematical statistics},
	title = {Statistics for spatial data},
	isbn = {978-0-471-84336-8},
	publisher = {Wiley},
	author = {Cressie, Noel A. C.},
	year = {1993},
	keywords = {Spatial analysis (Statistics)},
	file = {Cressie - 1993 - Statistics for spatial data.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Cressie - 1993 - Statistics for spatial data.pdf:application/pdf}
}

@misc{noauthor_stationary_2019,
	title = {Stationary process},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Stationary_process&oldid=887919881},
	abstract = {In mathematics and statistics, a stationary process (a.k.a. a strict/strictly stationary process or strong/strongly stationary process) is a stochastic process whose unconditional joint probability distribution does not change when shifted in time. Consequently, parameters such as mean and variance also do not change over time.
Since stationarity is an assumption underlying many statistical procedures used in time series analysis, non-stationary data is often transformed to become stationary. The most common cause of violation of stationarity is a trend in the mean, which can be due either to the presence of a unit root or of a deterministic trend. In the former case of a unit root, stochastic shocks have permanent effects, and the process is not mean-reverting. In the latter case of a deterministic trend, the process is called a trend stationary process, and stochastic shocks have only transitory effects after which the variable tends toward a deterministically evolving (non-constant) mean.
A trend stationary process is not strictly stationary, but can easily be transformed into a stationary process by removing the underlying trend, which is solely a function of time. Similarly, processes with one or more unit roots can be made stationary through differencing. An important type of non-stationary process that does not include a trend-like behavior is a cyclostationary process, which is a stochastic process that varies cyclically with time.
For many applications strict-sense stationarity is too restrictive. Other forms of stationarity such as wide-sense stationarity or N-th order stationarity are then employed. The definitions for different kinds of stationarity are not consistent among different authors (see Other terminology).},
	language = {en},
	urldate = {2019-05-02},
	journal = {Wikipedia},
	month = mar,
	year = {2019},
	note = {Page Version ID: 887919881},
	file = {Snapshot:/Users/adrian/Zotero/storage/GH5L2HT4/index.html:text/html}
}

@article{zhu_model_2019,
	title = {Model error correction in data assimilation by integrating neural networks},
	volume = {2},
	issn = {2096-0654},
	url = {https://ieeexplore.ieee.org/document/8665726/},
	doi = {10.26599/BDMA.2018.9020033},
	abstract = {In this paper, we suggest a new methodology which combines Neural Networks (NN) into Data Assimilation (DA). Focusing on the structural model uncertainty, we propose a framework for integration NN with the physical models by DA algorithms, to improve both the assimilation process and the forecasting results. The NNs are iteratively trained as observational data is updated. The main DA models used here are the Kalman filter and the variational approaches. The effectiveness of the proposed algorithm is validated by examples and by a sensitivity study.},
	language = {en},
	number = {2},
	urldate = {2019-05-06},
	journal = {Big Data Mining and Analytics},
	author = {Zhu, Jiangcheng and Hu, Shuang and Arcucci, Rossella and Xu, Chao and Zhu, Jihong and Guo, Yi-ke},
	month = jun,
	year = {2019},
	pages = {83--91},
	file = {Zhu et al. - 2019 - Model error correction in data assimilation by int.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Data Assimilation/Zhu et al. - 2019 - Model error correction in data assimilation by int.pdf:application/pdf}
}

@article{arcucci_effective_2018,
	title = {Effective variational data assimilation in air-pollution prediction},
	volume = {1},
	issn = {2096-0654},
	url = {https://ieeexplore.ieee.org/document/8400446/},
	doi = {10.26599/BDMA.2018.9020025},
	abstract = {Numerical simulations are widely used as a predictive tool to better understand complex air flows and pollution transport on the scale of individual buildings, city blocks, and entire cities. To improve prediction for air flows and pollution transport, we propose a Variational Data Assimilation (VarDA) model which assimilates data from sensors into the open-source, finite-element, fluid dynamics model Fluidity. VarDA is based on the minimization of a function which estimates the discrepancy between numerical results and observations assuming that the two sources of information, forecast and observations, have errors that are adequately described by error covariance matrices. The conditioning of the numerical problem is dominated by the condition number of the background error covariance matrix which is ill-conditioned. In this paper, a preconditioned VarDA model is presented, it is based on a reduced background error covariance matrix. The Empirical Orthogonal Functions (EOFs) method is used to alleviate the computational cost and reduce the space dimension. Experimental results are provided assuming observed values provided by sensors from positions mainly located on roofs of buildings.},
	language = {en},
	number = {4},
	urldate = {2019-05-06},
	journal = {Big Data Mining and Analytics},
	author = {Arcucci, Rossella and Pain, Christopher and Guo, Yi-Ke},
	month = dec,
	year = {2018},
	pages = {297--307},
	file = {Arcucci et al. - 2018 - Effective variational data assimilation in air-pol.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Data Assimilation/Arcucci et al. - 2018 - Effective variational data assimilation in air-pol.pdf:application/pdf}
}

@incollection{monestiez_advances_2001,
	address = {Dordrecht},
	title = {Advances in {Modeling} and {Inference} for {Environmental} {Processes} with {Nonstationary} {Spatial} {Covariance}},
	volume = {11},
	isbn = {978-0-7923-7107-6 978-94-010-0810-5},
	url = {http://www.springerlink.com/index/10.1007/978-94-010-0810-5_2},
	urldate = {2019-05-16},
	booktitle = {{geoENV} {III} {\textemdash} {Geostatistics} for {Environmental} {Applications}},
	publisher = {Springer Netherlands},
	author = {Sampson, P. D. and Damian, D. and Guttorp, P.},
	editor = {Monestiez, Pascal and Allard, Denis and Froidevaux, Roland},
	year = {2001},
	doi = {10.1007/978-94-010-0810-5_2},
	pages = {17--32},
	file = {Sampson et al. - 2001 - Advances in Modeling and Inference for Environment.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Sampson et al. - 2001 - Advances in Modeling and Inference for Environment.pdf:application/pdf}
}

@incollection{guttorp_20_1994,
	title = {20 {Methods} for estimating heterogeneous spatial covariance functions with environmental applications},
	volume = {12},
	isbn = {978-0-444-89803-6},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169716105800227},
	abstract = {Estimation of spatial covariance is important to many statistical problems in the analysis of environmental monitoring data. In this Chapter we review several difrerent methods for spati.al covariance estimation from monitoring data, with emphasis on methods for heterogeneous models. We briefly describe some applications, and outline how these methods can be extended to space-time and multivariate models.},
	language = {en},
	urldate = {2019-05-16},
	booktitle = {Handbook of {Statistics}},
	publisher = {Elsevier},
	author = {Guttorp, Peter and Sampson, Paul D.},
	year = {1994},
	doi = {10.1016/S0169-7161(05)80022-7},
	pages = {661--689},
	file = {Guttorp and Sampson - 1994 - 20 Methods for estimating heterogeneous spatial co.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Guttorp and Sampson - 1994 - 20 Methods for estimating heterogeneous spatial co.pdf:application/pdf}
}

@article{liu_when_2018,
	title = {When {Gaussian} {Process} {Meets} {Big} {Data}: {A} {Review} of {Scalable} {GPs}},
	shorttitle = {When {Gaussian} {Process} {Meets} {Big} {Data}},
	url = {http://arxiv.org/abs/1807.01065},
	abstract = {The vast quantity of information brought by big data as well as the evolving computer hardware encourages success stories in the machine learning community. In the meanwhile, it poses challenges for the Gaussian process (GP) regression, a well-known non-parametric and interpretable Bayesian model, which suffers from cubic complexity to data size. To improve the scalability while retaining desirable prediction quality, a variety of scalable GPs have been presented. But they have not yet been comprehensively reviewed and analyzed in order to be well understood by both academia and industry. The review of scalable GPs in the GP community is timely and important due to the explosion of data size. To this end, this paper is devoted to the review on state-of-the-art scalable GPs involving two main categories: global approximations which distillate the entire data and local approximations which divide the data for subspace learning. Particularly, for global approximations, we mainly focus on sparse approximations comprising prior approximations which modify the prior but perform exact inference, posterior approximations which retain exact prior but perform approximate inference, and structured sparse approximations which exploit specific structures in kernel matrix; for local approximations, we highlight the mixture/product of experts that conducts model averaging from multiple local experts to boost predictions. To present a complete review, recent advances for improving the scalability and capability of scalable GPs are reviewed. Finally, the extensions and open issues regarding the implementation of scalable GPs in various scenarios are reviewed and discussed to inspire novel ideas for future research avenues.},
	language = {en},
	urldate = {2019-05-16},
	journal = {arXiv:1807.01065 [cs, stat]},
	author = {Liu, Haitao and Ong, Yew-Soon and Shen, Xiaobo and Cai, Jianfei},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.01065},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Liu et al. - 2018 - When Gaussian Process Meets Big Data A Review of .pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Scalability/Liu et al. - 2018 - When Gaussian Process Meets Big Data A Review of .pdf:application/pdf}
}

@article{sampson_nonparametric_1992,
	title = {Nonparametric {Estimation} of {Nonstationary} {Spatial} {Covariance} {Structure}},
	volume = {87},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1992.10475181},
	doi = {10.1080/01621459.1992.10475181},
	language = {en},
	number = {417},
	urldate = {2019-05-28},
	journal = {Journal of the American Statistical Association},
	author = {Sampson, Paul D. and Guttorp, Peter},
	month = mar,
	year = {1992},
	pages = {108--119},
	file = {Sampson and Guttorp - 1992 - Nonparametric Estimation of Nonstationary Spatial .pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Sampson and Guttorp - 1992 - Nonparametric Estimation of Nonstationary Spatial .pdf:application/pdf}
}

@article{vetter_efficient_2014,
	title = {Efficient {Approximation} of the {Spatial} {Covariance} {Function} for {Large} {Datasets}},
	abstract = {Linear mixed effects models have been widely used in the spatial analysis of environmental processes. However, parameter estimation and spatial predictions involve the inversion and determinant of the n {\texttimes} n dimensional spatial covariance matrix of the data process, with n being the number of observations. Nowadays environmental variables are typically obtained through remote sensing and contain observations of the order of tens or hundreds of thousands on a single day, which quickly leads to bottlenecks in terms of computation speed and requirements in working memory. Therefore techniques for reducing the dimension of the problem are required. The present work analyzes approaches to approximate the spatial covariance function in a real dataset of remotely sensed carbon dioxide concentrations, obtained from the Atmospheric Infrared Sounder of NASA{\textquoteright}s {\textquotedblright}Aqua{\textquotedblright} satellite on the 1st of May 2009. In a cross-validation case study it is shown how fixed rank kriging, stationary covariance tapering and the full-scale approximation are able to notably speed up calculations. However, the loss in predictive performance caused by the approximation strongly differs. The best results were obtained for the full-scale approximation, which was able to overcome the individual weaknesses of the fixed rank kriging and the covariance tapering.},
	language = {en},
	author = {Vetter, Patrick and Schmid, Wolfgang},
	month = aug,
	year = {2014},
	pages = {36},
	file = {Vetter and Schmid - 2014 - Efficient Approximation of the Spatial Covariance .pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Vetter and Schmid - 2014 - Efficient Approximation of the Spatial Covariance .pdf:application/pdf}
}

@article{pourahmadi_covariance_2011,
	title = {Covariance {Estimation}: {The} {GLM} and {Regularization} {Perspectives}},
	volume = {26},
	issn = {0883-4237},
	shorttitle = {Covariance {Estimation}},
	url = {http://arxiv.org/abs/1202.1661},
	doi = {10.1214/11-STS358},
	abstract = {Finding an unconstrained and statistically interpretable reparameterization of a covariance matrix is still an open problem in statistics. Its solution is of central importance in covariance estimation, particularly in the recent high-dimensional data environment where enforcing the positive-definiteness constraint could be computationally expensive. We provide a survey of the progress made in modeling covariance matrices from two relatively complementary perspectives: (1) generalized linear models (GLM) or parsimony and use of covariates in low dimensions, and (2) regularization or sparsity for high-dimensional data. An emerging, unifying and powerful trend in both perspectives is that of reducing a covariance estimation problem to that of estimating a sequence of regression problems. We point out several instances of the regression-based formulation. A notable case is in sparse estimation of a precision matrix or a Gaussian graphical model leading to the fast graphical LASSO algorithm. Some advantages and limitations of the regression-based Cholesky decomposition relative to the classical spectral (eigenvalue) and variance-correlation decompositions are highlighted. The former provides an unconstrained and statistically interpretable reparameterization, and guarantees the positive-definiteness of the estimated covariance matrix. It reduces the unintuitive task of covariance estimation to that of modeling a sequence of regressions at the cost of imposing an a priori order among the variables. Elementwise regularization of the sample covariance matrix such as banding, tapering and thresholding has desirable asymptotic properties and the sparse estimated covariance matrix is positive definite with probability tending to one for large samples and dimensions.},
	language = {en},
	number = {3},
	urldate = {2019-05-30},
	journal = {Statistical Science},
	author = {Pourahmadi, Mohsen},
	month = aug,
	year = {2011},
	note = {arXiv: 1202.1661},
	keywords = {Statistics - Methodology},
	pages = {369--387},
	file = {Pourahmadi - 2011 - Covariance Estimation The GLM and Regularization .pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Pourahmadi - 2011 - Covariance Estimation The GLM and Regularization .pdf:application/pdf}
}

@article{furrer_covariance_2006,
	title = {Covariance {Tapering} for {Interpolation} of {Large} {Spatial} {Datasets}},
	volume = {15},
	issn = {1061-8600, 1537-2715},
	url = {http://www.tandfonline.com/doi/abs/10.1198/106186006X132178},
	doi = {10.1198/106186006X132178},
	language = {en},
	number = {3},
	urldate = {2019-05-30},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Furrer, Reinhard and Genton, Marc G and Nychka, Douglas},
	month = sep,
	year = {2006},
	pages = {502--523},
	file = {Furrer et al. - 2006 - Covariance Tapering for Interpolation of Large Spa.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Furrer et al. - 2006 - Covariance Tapering for Interpolation of Large Spa.pdf:application/pdf}
}

@article{sun_geostatistics_2012,
	title = {Geostatistics for {Large} {Datasets}},
	abstract = {We review various approaches for the geostatistical analysis of large datasets. First, we consider covariance structures that yield computational simplifications in geostatistics and briefly discuss how to test the suitability of such structures. Second, we describe the use of covariance tapering for both estimation and kriging purposes. Third, we consider likelihood approximations in both spatial and spectral domains. Fourth, we explore methods based on latent processes, such as Gaussian predictive processes and fixed rank kriging. Fifth, we describe methods based on Gaussian Markov random field approximations. Finally, we discuss multivariate extensions and open problems in this area.},
	language = {en},
	author = {Sun, Ying and Li, Bo and Genton, Marc G},
	year = {2012},
	pages = {23},
	file = {Sun et al. - 2012 - Geostatistics for Large Datasets.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Sun et al. - 2012 - Geostatistics for Large Datasets.pdf:application/pdf}
}

@article{cressie_fixed_2008,
	title = {Fixed rank kriging for very large spatial data sets: {Fixed} {Rank} {Kriging}},
	volume = {70},
	issn = {13697412},
	shorttitle = {Fixed rank kriging for very large spatial data sets},
	url = {http://doi.wiley.com/10.1111/j.1467-9868.2007.00633.x},
	doi = {10.1111/j.1467-9868.2007.00633.x},
	abstract = {Spatial statistics forvery largespatial data sets ischallenging. The size of the data set, n, causes problems incomputing optimal spatial predictors such as kriging, since itscomputa tional cost isof order A73.Inaddition, a large data set isoften defined on a large spatial domain, so the spatial process of interest typically exhibits non-stationary behaviour over that domain. A flexible familyof non-stationary covariance functions isdefined by using a set of basis functions that is fixed innumber, which leads to a spatial prediction method thatwe call fixed rankkriging. Specifically, fixed rankkriging iskrigingwithin this class of non-stationary covariance functions. It relies on computational simplifications when n is very large, for obtaining the spatial best linearunbiased predictor and itsmean-squared prediction error fora hidden spatial process. A method based on minimizing a weighted Frobenius norm yields best estimators of the covari ance function parameters, which are then substituted into the fixed rank kriging equations. The new methodology isapplied to a very large data set of total column ozone data, observed over the entire globe, where n isof the order of hundreds of thousands.},
	language = {en},
	number = {1},
	urldate = {2019-05-30},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Cressie, Noel and Johannesson, Gardar},
	month = jan,
	year = {2008},
	pages = {209--226},
	file = {Cressie and Johannesson - 2008 - Fixed rank kriging for very large spatial data set.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Cressie and Johannesson - 2008 - Fixed rank kriging for very large spatial data set.pdf:application/pdf}
}

@article{sang_full_2012,
	title = {A full scale approximation of covariance functions for large spatial data sets: {Approximation} of {Covariance} {Functions} for {Large} {Data} {Sets}},
	volume = {74},
	issn = {13697412},
	shorttitle = {A full scale approximation of covariance functions for large spatial data sets},
	url = {http://doi.wiley.com/10.1111/j.1467-9868.2011.01007.x},
	doi = {10.1111/j.1467-9868.2011.01007.x},
	abstract = {Gaussian process models have been widely used in spatial statistics but face tremendous computational challenges for very large data sets. The model fitting and spatial prediction of such models typically require O(n3) operations for a data set of size n. Various approximations of the covariance functions have been introduced to reduce the computational cost. However, most existing approximations can not simultaneously capture both the large and small scale spatial dependence. A new approximation scheme is developed in this paper to provide a high quality approximation to the covariance function at both the large and small spatial scales. The new approximation is the summation of two parts: a reduced rank covariance and a compactly supported covariance obtained by tapering the covariance of the residual of the reduced rank approximation. While the former part mainly captures the large scale spatial variation, the latter part captures the small scale, local variation that is unexplained by the former part. By combining the reduced rank representation and sparse matrix techniques, our approach allows for efficient computation for maximum likelihood estimation, spatial prediction and Bayesian inference. We illustrate the new approach with simulated and real data sets.},
	language = {en},
	number = {1},
	urldate = {2019-05-30},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Sang, Huiyan and Huang, Jianhua Z.},
	month = jan,
	year = {2012},
	pages = {111--132},
	file = {Sang and Huang - 2012 - A full scale approximation of covariance functions.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Sang and Huang - 2012 - A full scale approximation of covariance functions.pdf:application/pdf}
}

@article{rothman_positive_2012,
	title = {Positive definite estimators of large covariance matrices},
	volume = {99},
	issn = {0006-3444, 1464-3510},
	url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/ass025},
	doi = {10.1093/biomet/ass025},
	abstract = {Using convex optimization, we construct a sparse estimator of the covariance matrix that is positive definite and performs well in high-dimensional settings. A lasso-type penalty is used to encourage sparsity and a logarithmic barrier function is used to enforce positive definiteness. Consistency and convergence rate bounds are established as both the number of variables and sample size diverge. An efficient computational algorithm is developed and the merits of the approach are illustrated with simulations and a speech signal classification example.},
	language = {en},
	number = {3},
	urldate = {2019-05-30},
	journal = {Biometrika},
	author = {Rothman, A. J.},
	month = sep,
	year = {2012},
	pages = {733--740},
	file = {Rothman - 2012 - Positive definite estimators of large covariance m.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Rothman - 2012 - Positive definite estimators of large covariance m.pdf:application/pdf}
}

@article{rothman_sparse_2008,
	title = {Sparse permutation invariant covariance estimation},
	volume = {2},
	issn = {1935-7524},
	url = {http://projecteuclid.org/euclid.ejs/1214491853},
	doi = {10.1214/08-EJS176},
	abstract = {The paper proposes a method for constructing a sparse estimator for the inverse covariance (concentration) matrix in high-dimensional settings. The estimator uses a penalized normal likelihood approach and forces sparsity by using a lasso-type penalty. We establish a rate of convergence in the Frobenius norm as both data dimension p and sample size n are allowed to grow, and show that the rate depends explicitly on how sparse the true concentration matrix is. We also show that a correlationbased version of the method exhibits better rates in the operator norm. We also derive a fast iterative algorithm for computing the estimator, which relies on the popular Cholesky decomposition of the inverse but produces a permutation-invariant estimator. The method is compared to other estimators on simulated data and on a real data example of tumor tissue classification using gene expression data.},
	language = {en},
	number = {0},
	urldate = {2019-05-30},
	journal = {Electronic Journal of Statistics},
	author = {Rothman, Adam J. and Bickel, Peter J. and Levina, Elizaveta and Zhu, Ji},
	year = {2008},
	pages = {494--515},
	file = {Rothman et al. - 2008 - Sparse permutation invariant covariance estimation.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Rothman et al. - 2008 - Sparse permutation invariant covariance estimation.pdf:application/pdf}
}

@article{bailey_multiple_2019,
	title = {A multiple testing approach to the regularisation of large sample correlation matrices},
	volume = {208},
	issn = {03044076},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304407618302008},
	doi = {10.1016/j.jeconom.2018.10.006},
	abstract = {This paper proposes a regularisation method for the estimation of large covariance matrices that uses insights from the multiple testing (MT ) literature. The approach tests the statistical significance of individual pair-wise correlations and sets to zero those elements that are not statistically significant, taking account of the multiple testing nature of the problem. The effective p-values of the tests are set as a decreasing function of N (the cross section dimension), the rate of which is governed by the nature of dependence of the underlying observations, and the relative expansion rates of N and T (the time dimension). In this respect, the method specifies the appropriate thresholding parameter to be used under Gaussian and non-Gaussian settings. The MT estimator of the sample correlation matrix is shown to be consistent in the spectral and Frobenius norms, and in terms of support recovery, so long as the true covariance matrix is sparse. The performance of the proposed MT estimator is compared to a number of other estimators in the literature using Monte Carlo experiments. It is shown that the MT estimator performs well and tends to outperform the other estimators, particularly when N is larger than T .},
	language = {en},
	number = {2},
	urldate = {2019-05-30},
	journal = {Journal of Econometrics},
	author = {Bailey, Natalia and Pesaran, M. Hashem and Smith, L. Vanessa},
	month = feb,
	year = {2019},
	pages = {507--534},
	file = {Bailey et al. - 2019 - A multiple testing approach to the regularisation .pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Bailey et al. - 2019 - A multiple testing approach to the regularisation .pdf:application/pdf}
}

@article{cui_sparse_2016,
	title = {Sparse estimation of high-dimensional correlation matrices},
	volume = {93},
	issn = {01679473},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S016794731400293X},
	doi = {10.1016/j.csda.2014.10.001},
	abstract = {Several attempts to estimate covariance matrices with sparsity constraints have been made. A convex optimization formulation for estimating correlation matrices as opposed to covariance matrices is proposed. An efficient accelerated proximal gradient algorithm is developed, and it is shown that this method gives a faster rate of convergence. An adaptive version of this approach is also discussed. Simulation results and an analysis of a cardiovascular microarray confirm its performance and usefulness.},
	language = {en},
	urldate = {2019-05-30},
	journal = {Computational Statistics \& Data Analysis},
	author = {Cui, Ying and Leng, Chenlei and Sun, Defeng},
	month = jan,
	year = {2016},
	pages = {390--403},
	file = {Cui et al. - 2016 - Sparse estimation of high-dimensional correlation .pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Cui et al. - 2016 - Sparse estimation of high-dimensional correlation .pdf:application/pdf}
}

@article{maurya_joint_2014,
	title = {A joint convex penalty for inverse covariance matrix estimation},
	volume = {75},
	issn = {01679473},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167947314000267},
	doi = {10.1016/j.csda.2014.01.015},
	abstract = {The paper proposes a joint convex penalty for estimating the Gaussian inverse covariance matrix. A proximal gradient method is developed to solve the resulting optimization problem with more than one penalty constraints. The analysis shows that imposing a single constraint is not enough and the estimator can be improved by a trade-off between two convex penalties. The developed framework can be extended to solve wide arrays of constrained convex optimization problems. A simulation study is carried out to compare the performance of the proposed method to graphical lasso and the SPICE estimate of the inverse covariance matrix.},
	language = {en},
	urldate = {2019-05-30},
	journal = {Computational Statistics \& Data Analysis},
	author = {Maurya, Ashwini},
	month = jul,
	year = {2014},
	pages = {15--27},
	file = {Maurya - 2014 - A joint convex penalty for inverse covariance matr.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Maurya - 2014 - A joint convex penalty for inverse covariance matr.pdf:application/pdf}
}

@article{cressie_fitting_1985,
	title = {Fitting variogram models by weighted least squares},
	volume = {17},
	issn = {0020-5958, 1573-8868},
	url = {http://link.springer.com/10.1007/BF01032109},
	doi = {10.1007/BF01032109},
	abstract = {The method of weighted least squares is shown to be an appropriate way of fitting variogram models. The weighting scheme automatically gives most weight to early lags and downweights those lags with a small number o f pairs. Although weights are derived assuming the data are Gaussian (normal), they are shown to be still appropriate in the setting where data are a (smooth) transform o f the Gaussian case. The method o f (iterated) generalized least squares, which takes into account correlation between variogram estimators at different lags, offer more statistical efficiency at the price of more complexity. Weighted least squares for the robust estimator, based on square root differences, is less o f a compromise.},
	language = {en},
	number = {5},
	urldate = {2019-06-01},
	journal = {Journal of the International Association for Mathematical Geology},
	author = {Cressie, Noel},
	month = jul,
	year = {1985},
	pages = {563--586},
	file = {Cressie - 1985 - Fitting variogram models by weighted least squares.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Cressie - 1985 - Fitting variogram models by weighted least squares.pdf:application/pdf}
}

@book{wikle_spatio-temporal_2019,
	address = {Boca Raton, Florida : CRC Press, [2019]},
	edition = {1},
	title = {Spatio-{Temporal} {Statistics} with {R}},
	isbn = {978-1-351-76972-3},
	url = {https://www.taylorfrancis.com/books/9780429649783},
	language = {en},
	urldate = {2019-06-01},
	publisher = {Chapman and Hall/CRC},
	author = {Wikle, Christopher K. and Zammit-Mangion, Andrew and Cressie, Noel},
	month = feb,
	year = {2019},
	doi = {10.1201/9781351769723},
	file = {Wikle et al. - 2019 - Spatio-Temporal Statistics with R.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Wikle et al. - 2019 - Spatio-Temporal Statistics with R.pdf:application/pdf}
}

@article{haas_kriging_1990,
	title = {Kriging and automated variogram modeling within a moving window},
	volume = {24},
	issn = {09601686},
	url = {https://linkinghub.elsevier.com/retrieve/pii/096016869090508K},
	doi = {10.1016/0960-1686(90)90508-K},
	abstract = {A spatial estimation procedure based on ordinary kriging is described and evaluated which consists of using only samplingsitescontained within a movingwindowcentered at the estimate location for modeling the covariance structure and constructing the kriging equations. The moving window, by depending on local data only to estimate the spatial covariance structure and calculate the estimate, is less affected by spatial trend in the data than conventional krigingapproaches and implicitlymodels covariance nonstationarity. The window's covariance structure is estimated by automatically fitting a spherical variogram model to the unbiased estimatesof semi-variancecalculatedat severallags.The automatic fit uses nonlinear least squares regression constrained by the nugget parameter being nonnegative.},
	language = {en},
	number = {7},
	urldate = {2019-06-01},
	journal = {Atmospheric Environment. Part A. General Topics},
	author = {Haas, Timothy C.},
	month = jan,
	year = {1990},
	pages = {1759--1769},
	file = {Haas - 1990 - Kriging and automated variogram modeling within a .pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Haas - 1990 - Kriging and automated variogram modeling within a .pdf:application/pdf}
}

@misc{magic_magic_nodate,
	title = {{MAGIC}, {Envisaging}  a world with greener cities},
	url = {http://magic-air.uk/attorneys.html},
	abstract = {Managing Air in Green Inner Cities- MAGIC - is an EPSRC collaborative project between The University of Cambridge, University of Surrey and Imperial College London examining how we can create cities with no air pollution or urban-heat islands by 2050, to help combat climate change.},
	language = {en},
	urldate = {2019-06-03},
	journal = {Magic Air},
	author = {MAGIC},
	file = {Snapshot:/Users/adrian/Zotero/storage/6IANT66M/attorneys.html:text/html}
}

@book{rasmussen_gaussian_2006,
	address = {Cambridge, Mass},
	series = {Adaptive computation and machine learning},
	title = {Gaussian processes for machine learning},
	isbn = {978-0-262-18253-9},
	language = {en},
	publisher = {MIT Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	year = {2006},
	note = {OCLC: ocm61285753},
	keywords = {Data processing, Gaussian processes, Machine learning, Mathematical models},
	file = {RW.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/RW.pdf:application/pdf}
}

@article{ohagan_curve_1978,
	title = {Curve {Fitting} and {Optimal} {Design} for {Prediction}},
	volume = {40},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2984861},
	abstract = {[The optimal design problem is tackled in the framework of a new model and new objectives. A regression model is proposed in which the regression function is permitted to take any form over the space X of independent variables. The design objective is based on fitting a simplified function for prediction. The approach is Bayesian throughout. The new designs are more robust than conventional ones. They also avoid the need to limit artificially design points to a predetermined subset of X. New solutions are also offered for the problems of smoothing, curve fitting and the selection of regressor variables.]},
	number = {1},
	urldate = {2019-06-04},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {O'Hagan, A.},
	year = {1978},
	pages = {1--42}
}

@article{deisenroth_mathematics_2018,
	title = {Mathematics for {Machine} {Learning}},
	language = {en},
	author = {Deisenroth, Marc Peter and Faisal, Aldo and Ong, Cheng Soon},
	year = {2018},
	pages = {43},
	file = {chapter06.pdf:/Users/adrian/Documents/2018/Imperial College /T1/496 - Mathematics for Machine Learning/MML Book/chapter06.pdf:application/pdf}
}

@book{cover_elements_1991,
	address = {New York},
	series = {Wiley series in telecommunications},
	title = {Elements of information theory},
	isbn = {978-0-471-06259-2},
	language = {en},
	publisher = {Wiley},
	author = {Cover, T. M. and Thomas, Joy A.},
	year = {1991},
	keywords = {Information theory},
	file = {Cover and Thomas - 1991 - Elements of information theory.pdf:/Users/adrian/Zotero/storage/8YJTWRSR/Cover and Thomas - 1991 - Elements of information theory.pdf:application/pdf}
}

@article{johnstone_distribution_2001,
	title = {On the distribution of the largest eigenvalue in principal components analysis},
	volume = {29},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1009210544},
	doi = {10.1214/aos/1009210544},
	abstract = {Let x(1) denote the square of the largest singular value of an n {\texttimes} p matrix X, all of whose entries are independent standard Gaussian variates. Equivalently, x(1) is the largest principal component variance of the covariance matrix X'XX'XX'X, or the largest eigenvalue of a p-variate Wishart distribution on n degrees of freedom with identity covariance. Consider the limit of large p and n with n/p=$\gamma$>=1n/p=$\gamma$>=1n/p = {\textbackslash}gamma {\textbackslash}ge 1. When centered by $\mu$p=(n-1-----?+p-?)2$\mu$p=(n-1+p)2{\textbackslash}mu\_p = ({\textbackslash}sqrt\{n-1\} + {\textbackslash}sqrt\{p\}){\textasciicircum}2 and scaled by $\sigma$p=(n-1-----?+p-?)(1/n-1-----?+1/p-?1/3$\sigma$p=(n-1+p)(1/n-1+1/p1/3{\textbackslash}sigma\_p = ({\textbackslash}sqrt\{n-1\} + {\textbackslash}sqrt\{p\})(1/{\textbackslash}sqrt\{n-1\} + 1/{\textbackslash}sqrt\{p\}{\textasciicircum}\{1/3\}, the distribution of x(1) approaches the Tracey-Widom law of order 1, which is defined in terms of the Painlev{\'e} II differential equation and can be numerically evaluated and tabulated in software. Simulations show the approximation to be informative for n and p as small as 5. The limit is derived via a corresponding result for complex Wishart matrices using methods from random matrix theory. The result suggests that some aspects of large p multivariate distribution theory may be easier to apply in practice than their fixed p counterparts.},
	language = {en},
	number = {2},
	urldate = {2019-06-07},
	journal = {The Annals of Statistics},
	author = {Johnstone, Iain M.},
	month = apr,
	year = {2001},
	mrnumber = {MR1863961},
	zmnumber = {1016.62078},
	keywords = {empirical orthogonal functions, Fredholm determinant, Karhunen{\textendash}Lo{\`e}ve transform, Laguerre ensemble, Laguerre polynomial, largest eigenvalue, largest singular value, Liouville{\textendash}Green method, Painlev{\'e} equation, Plancherel{\textendash}Rotach asymptotics, random matrix theory, Tracy{\textendash}Widom distribution, Wishart distribution},
	pages = {295--327},
	file = {Johnstone - 2001 - On the distribution of the largest eigenvalue in p.pdf:/Users/adrian/Zotero/storage/5WMCTC4L/Johnstone - 2001 - On the distribution of the largest eigenvalue in p.pdf:application/pdf;Snapshot:/Users/adrian/Zotero/storage/R6EFVYUQ/1009210544.html:text/html}
}

@article{fan_overview_2015,
	title = {An {Overview} on the {Estimation} of {Large} {Covariance} and {Precision} {Matrices}},
	url = {http://arxiv.org/abs/1504.02995},
	abstract = {Estimating large covariance and precision matrices are fundamental in modern multivariate analysis. The problems arise from statistical analysis of large panel economics and finance data. The covariance matrix reveals marginal correlations between variables, while the precision matrix encodes conditional correlations between pairs of variables given the remaining variables. In this paper, we provide a selective review of several recent developments on estimating large covariance and precision matrices. We focus on two general approaches: rank based method and factor model based method. Theories and applications of both approaches are presented. These methods are expected to be widely applicable to analysis of economic and financial data.},
	urldate = {2019-06-07},
	journal = {arXiv:1504.02995 [stat]},
	author = {Fan, Jianqing and Liao, Yuan and Liu, Han},
	month = apr,
	year = {2015},
	note = {arXiv: 1504.02995},
	keywords = {Statistics - Methodology},
	file = {arXiv.org Snapshot:/Users/adrian/Zotero/storage/KB6ZSBNF/1504.html:text/html;Fan et al. - 2015 - An Overview on the Estimation of Large Covariance .pdf:/Users/adrian/Zotero/storage/8HK8IWTU/Fan et al. - 2015 - An Overview on the Estimation of Large Covariance .pdf:application/pdf}
}

@incollection{paciorek_nonstationary_2004,
	title = {Nonstationary {Covariance} {Functions} for {Gaussian} {Process} {Regression}},
	url = {http://papers.nips.cc/paper/2350-nonstationary-covariance-functions-for-gaussian-process-regression.pdf},
	urldate = {2019-06-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 16},
	publisher = {MIT Press},
	author = {Paciorek, Christopher J. and Schervish, Mark J.},
	editor = {Thrun, S. and Saul, L. K. and Sch{\"o}lkopf, B.},
	year = {2004},
	pages = {273--280},
	file = {NIPS Snapshot:/Users/adrian/Zotero/storage/QTG3FZ2T/2350-nonstationary-covariance-functions-for-gaussian-process-regression.html:text/html;Paciorek and Schervish - 2004 - Nonstationary Covariance Functions for Gaussian Pr.pdf:/Users/adrian/Zotero/storage/BZB7NFYR/Paciorek and Schervish - 2004 - Nonstationary Covariance Functions for Gaussian Pr.pdf:application/pdf}
}

@article{heaton_case_2018,
	title = {A {Case} {Study} {Competition} {Among} {Methods} for {Analyzing} {Large} {Spatial} {Data}},
	issn = {1085-7117, 1537-2693},
	url = {http://link.springer.com/10.1007/s13253-018-00348-w},
	doi = {10.1007/s13253-018-00348-w},
	language = {en},
	urldate = {2019-06-10},
	journal = {Journal of Agricultural, Biological and Environmental Statistics},
	author = {Heaton, Matthew J. and Datta, Abhirup and Finley, Andrew O. and Furrer, Reinhard and Guinness, Joseph and Guhaniyogi, Rajarshi and Gerber, Florian and Gramacy, Robert B. and Hammerling, Dorit and Katzfuss, Matthias and Lindgren, Finn and Nychka, Douglas W. and Sun, Furong and Zammit-Mangion, Andrew},
	month = dec,
	year = {2018},
	file = {Heaton et al. - 2018 - A Case Study Competition Among Methods for Analyzi.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Heaton et al. - 2018 - A Case Study Competition Among Methods for Analyzi.pdf:application/pdf}
}

@article{titsias_variational_2009,
	title = {Variational {Learning} of {Inducing} {Variables} in {Sparse} {Gaussian} {Processes}},
	abstract = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs are defined to be variational parameters which are selected by minimizing the Kullback-Leibler divergence between the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.},
	language = {en},
	author = {Titsias, Michalis K},
	year = {2009},
	pages = {8},
	file = {Titsias - Variational Learning of Inducing Variables in Spar.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Titsias - Variational Learning of Inducing Variables in Spar.pdf:application/pdf}
}

@article{yu_stochastic_2017,
	title = {Stochastic {Variational} {Inference} for {Bayesian} {Sparse} {Gaussian} {Process} {Regression}},
	url = {http://arxiv.org/abs/1711.00221},
	abstract = {This paper presents a novel variational inference framework for deriving a family of Bayesian sparse Gaussian process regression (SGPR) models whose approximations are variationally optimal with respect to the full-rank GPR model enriched with various corresponding correlation structures of the observation noises. Our variational Bayesian SGPR (VBSGPR) models jointly treat both the distributions of the inducing variables and hyperparameters as variational parameters, which enables the decomposability of the variational lower bound that in turn can be exploited for stochastic optimization. Such a stochastic optimization involves iteratively following the stochastic gradient of the variational lower bound to improve its estimates of the optimal variational distributions of the inducing variables and hyperparameters (and hence the predictive distribution) of our VBSGPR models and is guaranteed to achieve asymptotic convergence to them. We show that the stochastic gradient is an unbiased estimator of the exact gradient and can be computed in constant time per iteration, hence achieving scalability to big data. We empirically evaluate the performance of our proposed framework on two real-world, massive datasets.},
	language = {en},
	urldate = {2019-06-12},
	journal = {arXiv:1711.00221 [cs, stat]},
	author = {Yu, Haibin and Hoang, Trong Nghia and Low, Kian Hsiang and Jaillet, Patrick},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.00221},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Yu et al. - 2017 - Stochastic Variational Inference for Bayesian Spar.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Yu et al. - 2017 - Stochastic Variational Inference for Bayesian Spar.pdf:application/pdf}
}

@article{titsias_variational_nodate,
	title = {Variational {Model} {Selection} for {Sparse} {Gaussian} {Process} {Regression}},
	abstract = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs are defined to be variational parameters which are selected by minimizing the Kullback-Leibler divergence between the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.},
	language = {en},
	author = {Titsias, Michalis K},
	pages = {20},
	file = {e492a629a98db7f9d77d552fd3568ff42189.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/e492a629a98db7f9d77d552fd3568ff42189.pdf:application/pdf}
}

@article{wilson_kernel_2015,
	title = {Kernel {Interpolation} for {Scalable} {Structured} {Gaussian} {Processes} ({KISS}-{GP})},
	abstract = {We introduce a new structured kernel interpolation (SKI) framework, which generalises and unifies inducing point methods for scalable Gaussian processes (GPs). SKI methods produce kernel approximations for fast computations through kernel interpolation. The SKI framework clarifies how the quality of an inducing point approach depends on the number of inducing (aka interpolation) points, interpolation strategy, and GP covariance kernel. SKI also provides a mechanism to create new scalable kernel methods, through choosing different kernel interpolation strategies. Using SKI, with local cubic kernel interpolation, we introduce KISSGP, which is 1) more scalable than inducing point alternatives, 2) naturally enables Kronecker and Toeplitz algebra for substantial additional gains in scalability, without requiring any grid data, and 3) can be used for fast and expressive kernel learning. KISS-GP costs O(n) time and storage for GP inference. We evaluate KISS-GP for kernel matrix approximation, kernel learning, and natural sound modelling.},
	language = {en},
	author = {Wilson, Andrew Gordon and Nickisch, Hannes},
	year = {2015},
	pages = {10},
	file = {Wilson and Nickisch - Kernel Interpolation for Scalable Structured Gauss.pdf:/Users/adrian/Zotero/storage/VQR8M9UI/Wilson and Nickisch - Kernel Interpolation for Scalable Structured Gauss.pdf:application/pdf}
}

@misc{gpy_gpy:_2014,
	title = {{GPy}: {Gaussian} process framework in python},
	url = {http://github.com/SheffieldML/GPy},
	author = {GPy},
	year = {2014}
}

@article{cressie_fixed_2008-1,
	title = {Fixed rank kriging for very large spatial data sets: {Fixed} {Rank} {Kriging}},
	volume = {70},
	issn = {13697412},
	shorttitle = {Fixed rank kriging for very large spatial data sets},
	url = {http://doi.wiley.com/10.1111/j.1467-9868.2007.00633.x},
	doi = {10.1111/j.1467-9868.2007.00633.x},
	abstract = {Spatial statistics for very large spatial data sets is challenging.The size of the data set, n, causes problems in computing optimal spatial predictors such as kriging, since its computational cost is of order n3. In addition, a large data set is often defined on a large spatial domain, so the spatial process of interest typically exhibits non-stationary behaviour over that domain. A flexible family of non-stationary covariance functions is defined by using a set of basis functions that is fixed in number, which leads to a spatial prediction method that we call fixed rank kriging. Specifically, fixed rank kriging is kriging within this class of non-stationary covariance functions. It relies on computational simplifications when n is very large, for obtaining the spatial best linear unbiased predictor and its mean-squared prediction error for a hidden spatial process. A method based on minimizing a weighted Frobenius norm yields best estimators of the covariance function parameters, which are then substituted into the fixed rank kriging equations. The new methodology is applied to a very large data set of total column ozone data, observed over the entire globe, where n is of the order of hundreds of thousands.},
	language = {en},
	number = {1},
	urldate = {2019-07-04},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Cressie, Noel and Johannesson, Gardar},
	month = jan,
	year = {2008},
	pages = {209--226},
	file = {Cressie and Johannesson - 2008 - Fixed rank kriging for very large spatial data set.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Cressie and Johannesson - 2008 - Fixed rank kriging for very large spatial data set 2.pdf:application/pdf}
}

@article{katzfuss_tutorial_nodate,
	title = {Tutorial on {Fixed} {Rank} {Kriging} ({FRK}) of {CO}2 {Data}},
	abstract = {In this document, we describe Fixed Rank Kriging (FRK), an approach to the analysis of very large spatial datasets. Such datasets now arise in many fields; our focus is on satellite measurements of CO2. FRK predictors and standard errors can be computed rapidly, even for datasets with a million or more observations. FRK relies on a so-called spatial random effects (SRE) model, which assumes that the process of interest can be expressed as a linear combination of spatial basis functions, plus a fine-scale-variation component. Here, we describe in detail all steps involved in the analysis of a spatial dataset using FRK, we illustrate the steps using a synthetic dataset, and we provide Matlab code on an accompanying website.},
	language = {en},
	author = {Katzfuss, Matthias and Cressie, Noel},
	pages = {24},
	file = {Katzfuss and Cressie - Tutorial on Fixed Rank Kriging (FRK) of CO2 Data.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Katzfuss and Cressie - Tutorial on Fixed Rank Kriging (FRK) of CO2 Data.pdf:application/pdf}
}

@article{foster_stable_2009,
	title = {Stable and {Efficient} {Gaussian} {Process} {Calculations}},
	abstract = {The use of Gaussian processes can be an effective approach to prediction in a supervised learning environment. For large data sets, the standard Gaussian process approach requires solving very large systems of linear equations and approximations are required for the calculations to be practical. We will focus on the subset of regressors approximation technique. We will demonstrate that there can be numerical instabilities in a well known implementation of the technique. We discuss alternate implementations that have better numerical stability properties and can lead to better predictions. Our results will be illustrated by looking at an application involving prediction of galaxy redshift from broadband spectrum data.},
	language = {en},
	author = {Foster, Leslie and Waagen, Alex and Aijaz, Nabeela and Hurley, Michael and Luis, Apolonio and Rinsky, Joel and Satyavolu, Chandrika and Com, Mailbolt},
	year = {2009},
	pages = {26},
	file = {Foster et al. - 2009 - Stable and Efficient Gaussian Process Calculations.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Foster et al. - 2009 - Stable and Efficient Gaussian Process Calculations.pdf:application/pdf}
}

@article{melkumyan_sparse_2009,
	title = {A {Sparse} {Covariance} {Function} for {Exact} {Gaussian} {Process} {Inference} in {Large} {Datasets}},
	abstract = {Despite the success of Gaussian processes (GPs) in modelling spatial stochastic processes, dealing with large datasets is still challenging. The problem arises by the need to invert a potentially large covariance matrix during inference. In this paper we address the complexity problem by constructing a new stationary covariance function (Mercer kernel) that naturally provides a sparse covariance matrix. The sparseness of the matrix is defined by hyperparameters optimised during learning. The new covariance function enables exact GP inference and performs comparatively to the squared-exponential one, at a lower computational cost. This allows the application of GPs to large-scale problems such as ore grade prediction in mining or 3D surface modelling. Experiments show that using the proposed covariance function, very sparse covariance matrices are normally obtained which can be effectively used for faster inference and less memory usage.},
	language = {en},
	author = {Melkumyan, Arman and Ramos, Fabio},
	year = {2009},
	pages = {7},
	file = {Melkumyan and Ramos - 2009 - A Sparse Covariance Function for Exact Gaussian Pr.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Gaussian Processes/Melkumyan and Ramos - 2009 - A Sparse Covariance Function for Exact Gaussian Pr.pdf:application/pdf}
}

@article{friedman_sparse_2008,
	title = {Sparse inverse covariance estimation with the graphical lasso},
	volume = {9},
	issn = {1465-4644, 1468-4357},
	url = {https://academic.oup.com/biostatistics/article-lookup/doi/10.1093/biostatistics/kxm045},
	doi = {10.1093/biostatistics/kxm045},
	abstract = {We consider the problem of estimating sparse graphs by a lasso penalty applied to the inverse covariance matrix. Using a coordinate descent procedure for the lasso, we develop a simple algorithm{\textemdash} the Graphical Lasso{\textemdash} that is remarkably fast: it solves a 1000 node problem (\~{} 500, 000 parameters) in at most a minute, and is 30 to 4000 times faster than competing methods. It also provides a conceptual link between the exact problem and the approximation suggested by Meinshausen \& Bu{\textasciidieresis}hlmann (2006). We illustrate the method on some cell-signaling data from proteomics.},
	language = {en},
	number = {3},
	urldate = {2019-07-15},
	journal = {Biostatistics},
	author = {Friedman, J. and Hastie, T. and Tibshirani, R.},
	month = jul,
	year = {2008},
	pages = {432--441},
	file = {Friedman et al. - 2008 - Sparse inverse covariance estimation with the grap.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Covariance/Friedman et al. - 2008 - Sparse inverse covariance estimation with the grap.pdf:application/pdf}
}

@article{pedregosa_scikit-learn:_2011,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	language = {en},
	journal = {MACHINE LEARNING IN PYTHON},
	author = {Pedregosa, Fabian and Varoquaux, Gael and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David},
	year = {2011},
	pages = {6},
	file = {Pedregosa et al. - Scikit-learn Machine Learning in Python.pdf:/Users/adrian/Zotero/storage/ECASTCSQ/Pedregosa et al. - Scikit-learn Machine Learning in Python.pdf:application/pdf}
}

@misc{noauthor_shapely_2019,
	title = {Shapely : {Manipulation} and analysis of geometric objects. {Contribute} to {Toblerity}/{Shapely} development by creating an account on {GitHub}},
	copyright = {View license},
	url = {https://github.com/Toblerity/Shapely},
	urldate = {2019-08-01},
	publisher = {Toblerity},
	month = aug,
	year = {2019},
	note = {original-date: 2011-12-31T19:43:11Z}
}

@article{chen_shrinkage_2010,
	title = {Shrinkage {Algorithms} for {MMSE} {Covariance} {Estimation}},
	volume = {58},
	issn = {1053-587X, 1941-0476},
	url = {http://ieeexplore.ieee.org/document/5484583/},
	doi = {10.1109/TSP.2010.2053029},
	abstract = {We address covariance estimation in the sense of minimum mean-squared error (MMSE) when the samples are Gaussian distributed. Specifically, we consider shrinkage methods which are suitable for high dimensional problems with a small number of samples (large p small n). First, we improve on the Ledoit-Wolf (LW) method by conditioning on a sufficient statistic. By the Rao-Blackwell theorem, this yields a new estimator called RBLW, whose mean-squared error dominates that of LW for Gaussian variables. Second, to further reduce the estimation error, we propose an iterative approach which approximates the clairvoyant shrinkage estimator. Convergence of this iterative method is established and a closed form expression for the limit is determined, which is referred to as the oracle approximating shrinkage (OAS) estimator. Both RBLW and OAS estimators have simple expressions and are easily implemented. Although the two methods are developed from different perspectives, their structure is identical up to specified constants. The RBLW estimator provably dominates the LW method for Gaussian samples. Numerical simulations demonstrate that the OAS approach can perform even better than RBLW, especially when n is much less than p. We also demonstrate the performance of these techniques in the context of adaptive beamforming.},
	language = {en},
	number = {10},
	urldate = {2019-08-06},
	journal = {IEEE Transactions on Signal Processing},
	author = {Chen, Yilun and Wiesel, Ami and Eldar, Yonina C. and Hero, Alfred O.},
	month = oct,
	year = {2010},
	pages = {5016--5029},
	file = {Chen et al. - 2010 - Shrinkage Algorithms for MMSE Covariance Estimatio.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Covariance/Chen et al. - 2010 - Shrinkage Algorithms for MMSE Covariance Estimatio.pdf:application/pdf}
}

@article{ledoit_honey_2003,
	title = {Honey, {I} {Shrunk} the {Sample} {Covariance} {Matrix}},
	abstract = {The central message of this paper is that nobody should be using the sample covariance matrix for the purpose of portfolio optimization. It contains estimation error of the kind most likely to perturb a mean-variance optimizer. In its place, we suggest using the matrix obtained from the sample covariance matrix through a transformation called shrinkage. This tends to pull the most extreme coefficients towards more central values, thereby systematically reducing estimation error where it matters most. Statistically, the challenge is to know the optimal shrinkage intensity, and we give the formula for that. Without changing any other step in the portfolio optimization process, we show on actual stock market data that shrinkage reduces tracking error relative to a benchmark index, and substantially increases the realized information ratio of the active portfolio manager.},
	language = {en},
	author = {Ledoit, Olivier and Wolf, Michael},
	year = {2003},
	pages = {22},
	file = {1bv0Vq-honey.pdf:/Users/adrian/Documents/2019/Imperial College/T3/Master Project.nosync/MScProject_OptSensors_GP/Covariance/1bv0Vq-honey.pdf:application/pdf}
}

@article{ledoit_improved_2003,
	title = {Improved estimation of the covariance matrix of stock returns with an application to portfolio selection},
	volume = {10},
	issn = {09275398},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0927539803000070},
	doi = {10.1016/S0927-5398(03)00007-0},
	abstract = {This paper proposes to estimate the covariance matrix of stock returns by an optimally weighted average of two existing estimators: the sample covariance matrix and single-index covariance matrix. This method is generally known as shrinkage, and it is standard in decision theory and in empirical Bayesian statistics. Our shrinkage estimator can be seen as a way to account for extra-market covariance without having to specify an arbitrary multifactor structure. For NYSE and AMEX stock returns from 1972 to 1995, it can be used to select portfolios with significantly lower out-of-sample variance than a set of existing estimators, including multifactor models.},
	language = {en},
	number = {5},
	urldate = {2019-08-16},
	journal = {Journal of Empirical Finance},
	author = {Ledoit, Olivier and Wolf, Michael},
	month = dec,
	year = {2003},
	pages = {603--621},
	file = {Ledoit and Wolf - 2003 - Improved estimation of the covariance matrix of st.pdf:/Users/adrian/Zotero/storage/LURX93XM/Ledoit and Wolf - 2003 - Improved estimation of the covariance matrix of st.pdf:application/pdf}
}

@article{ledoit_well-conditioned_2004,
	title = {A well-conditioned estimator for large-dimensional covariance matrices},
	volume = {88},
	issn = {0047259X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0047259X03000964},
	doi = {10.1016/S0047-259X(03)00096-4},
	abstract = {Many applied problems require a covariance matrix estimator that is not only invertible, but also well-conditioned (that is, inverting it does not amplify estimation error). For largedimensional covariance matrices, the usual estimator{\textemdash}the sample covariance matrix{\textemdash}is typically not well-conditioned and may not even be invertible. This paper introduces an estimator that is both well-conditioned and more accurate than the sample covariance matrix asymptotically. This estimator is distribution-free and has a simple explicit formula that is easy to compute and interpret. It is the asymptotically optimal convex linear combination of the sample covariance matrix with the identity matrix. Optimality is meant with respect to a quadratic loss function, asymptotically as the number of observations and the number of variables go to infinity together. Extensive Monte Carlo confirm that the asymptotic results tend to hold well in finite sample.},
	language = {en},
	number = {2},
	urldate = {2019-08-16},
	journal = {Journal of Multivariate Analysis},
	author = {Ledoit, Olivier and Wolf, Michael},
	month = feb,
	year = {2004},
	pages = {365--411},
	file = {Ledoit and Wolf - 2004 - A well-conditioned estimator for large-dimensional.pdf:/Users/adrian/Zotero/storage/G3DPKT6X/Ledoit and Wolf - 2004 - A well-conditioned estimator for large-dimensional.pdf:application/pdf}
}

@misc{noauthor_api_nodate,
	title = {{API} {Reference} {\textemdash} scikit-learn 0.21.3 documentation},
	url = {https://scikit-learn.org/stable/modules/classes.html#module-sklearn.covariance},
	urldate = {2019-08-17},
	file = {API Reference {\textemdash} scikit-learn 0.21.3 documentation:/Users/adrian/Zotero/storage/HUPER9BD/classes.html:text/html}
}

@article{hansen_truncatedsvd_1987,
	title = {The {truncatedSVD} as a method for regularization},
	volume = {27},
	issn = {0006-3835, 1572-9125},
	url = {http://link.springer.com/10.1007/BF01937276},
	doi = {10.1007/BF01937276},
	abstract = {I{\textquoteright}hc truncated singular value decomposition (SW) is considered as a method for rcgularization of ill-posed linear lcast squares pro\&ins. In particular, the .truncatcd SVl{\textgreater} solution is compared with the u s u a l rcguiarizcd so.lution. Ncccssary conditions arc d\&cd in which the two m\&hods will yield similar results. This investigation suggests the truncated SW as n filvorablc al-. tcrnativc to st;lndard-form rcgularization in cast of ill-conditioned matrices with a well-dctcrmincd rank.},
	language = {en},
	number = {4},
	urldate = {2019-08-17},
	journal = {BIT},
	author = {Hansen, Per Christian},
	month = dec,
	year = {1987},
	pages = {534--553},
	file = {Hansen - 1987 - The truncatedSVD as a method for regularization.pdf:/Users/adrian/Zotero/storage/KDK3I7C9/Hansen - 1987 - The truncatedSVD as a method for regularization.pdf:application/pdf}
}